---
layout: post
title: "ROUGE: 예시를 통해 ROUGE 성능 지표 이해하기"
subtitle: '텍스트 요약과 기계번역의 성능 평가에 자주 사용되는 ROUGE 스코어에 대해 알아보자'
author: "devfon"
header-style: text
lang: kr
tags:
  - AI
  - NLP
---

_본 글은 Northeastern University의 강의 자료 [**What is ROUGE and how it works for evaluation of
summarization tasks?**](http://www.ccs.neu.edu/home/vip/teach/DMcourse/5_topicmodel_summ/notes_slides/What-is-ROUGE.pdf)를 한국어로 옮긴 글입니다. 원문으로 읽고자 하신 분들은 링크를 참조해주세요._


## ROUGE가 요약 모델의 성능을 평가하는 방법

ROUGE는 텍스트 요약 모델의 성능을 평가하는 지표로 **Recall-Oriented Understudy for Gisting Evaluation**의 준말입니다. 본질적으로 ROUGE는 **텍스트 자동 요약** 및 **기계 번역**의 성능을 평가하는 지표들을 일컬으며, 모델이 생성한 요약본 혹은 번역본을 사람이 만들어 놓은 참조본과 대조해 성능 점수를 계산합니다. 우리가 다음과 같은 **시스템 요약**과 **참조 요약**을 가지고 있다고 가정해봅시다.

**시스템 요약 (모델 생성 요약)**:
> the cat was found under the bed

**참조 요약 (Gold standard: 대개 사람이 직접 만든 요약본)**:
> the cat was under the bed

만약 우리가 개별 단어에만 집중하고자 한다면 모델이 생성한 **시스템 요약**과 사람이 만들어 놓은 **참조 요약** 간 겹치는 단어는 총 6개입니다. 그러나 이 **6**이라는 숫자는 **성능 지표 (Metric)**로 바로 사용하기에 적합하지 않은 수입니다. 우리는 정량적 지표가 될 수 있는 값을 얻기 위해 6이라는 숫자를 이용해 **Recall**과 **Precision**을 계산할 수 있어야 합니다.

<br/>

## ROUGE에서의 Precision과 Recall
ROUGE에서의 **Precision**과 **Recall**은 아주 단순하게 구할 수 있는 점수입니다. **Recall**은 참조 요약본을 구성하는 단어들 중 몇 개의 단어가 시스템 요약본의 단어들과 겹치는지를 보고자 하는 점수입니다. 우리가 unigram을 하나의 단어로 사용한다고 하면, **Recall**은 다음과 같이 계산될 수 있습니다.

![](/img/in-post/equation1.png)

그리고 앞서 살펴본 예제에서의 **Recall** 점수는 다음과 같습니다.

![](/img/in-post/equation2.png)

이 점수의 의미는 **참조 요약본** 내 모든 unigram이 모델이 생성한 **시스템 요약본**에 등장했다는 것입니다. 언뜻 보기에 해당 점수는 무척 좋게 느껴질 수 있습니다. 그러나 해당 점수가 요약 성능의 모든 것을 대변해줄 수는 없습니다. 만약 모델이 생성했던 시스템 요약본이 **엄청나게 긴** 문장이었을 경우, 시스템 요약본이 참조 요약본과 크게 관련이 없을지라도 참조 요약본의 대부분의 단어를 포함할 가능성이 커지게 됩니다. 쉽게 이야기하자면 존재하는 모든 단어를 생성해내게 되면 참조 요약본을 구성하는 모든 단어를 포함할 수 있다는 것이죠.

이러한 문제를 해결해주기 위해 우리는 **Precision**이라는 점수를 필요로 하게 됩니다. **Precision**은 모델이 생생한 시스템 요약본 중 **참조 요약본**과 관련 있는 단어들이 얼마나 많이 존재하는지를 보고자 합니다. 이는 다음 식을 통해 구할 수 있습니다.

![](/img/in-post/equation3.png)

![](/img/in-post/equation4.png)

위 예제에 적용해보자면 **Precision**은 모델이 생성해 낸 문장을 구성하는 7개의 단어 중 6개의 단어만 실제 요약과 연관 있는 단어로 볼 수 있을 것입니다. 이번에는 시스템 요약본 문장을 다음과 같이 바꿔봅시다.

**시스템 요약 2:**
> the tiny little cat was found under the big funny bed

이제 **Precision** 점수는 다음과 같아집니다.

![](/img/in-post/equation5.png)

그렇게 좋은 결과 같아 보이지는 않네요. **Precision**의 관점에서 보면 모델은 참조 요약본과 최대한 정확한 단어들을 생성해야 합니다. 그러나 모델이 생성한 요약 문장에 불필요한 단어가 너무 많기 때문에 좋은 점수를 받을 수 없는 것이지요. 따라서 모델의 정확한 성능 평가를 위해서는 **Precision**과 **Recall**을 모두 계산한 후, **F-Measure**를 측정하는 것이 바람직합니다. 만약 당신이 여러 제약 조건들로 인해 어떻게든 **정확한** 요약을 만들어내야 한다면, **Recall**만을 사용하는 것이 좋을 것입니다.

<br/>

## ROUGE-N, ROUGE-S, 그리고 ROUGE-L은 무엇을 의미할까 ?
ROUGE-N, ROUGE-S, ROUGE-L은 각각 요약본의 일정 **부분**을 비교하는 것을 의미합니다. 예를 들어, ROUGE-1은 시스템 요약본과 참조 요약본 간 겹치는 Uni-gram의 수를 보고자 합니다. 그리고 ROUGE-2는 시스템 요약본과 참조 요약본 간 겹치는 Bi-gram의 수를 보고자 하는 것이고요. 위의 예시들을 다시 살펴보도록 합시다. 

우리는 현재 ROUGE-2를 이용해 Precision과 Recall 스코어를 계산하고자 하는 상황입니다.

**시스템 요약**:
> the cat was found under the bed

**참조 요약**:
> the cat was under the bed

**시스템 요약 (bi-grams)**:
> the cat, cat was, was found, found under, under the, the bed

**참조 요약 (bi-grams)**:
> the cat, cat was, was under, under the, the bed


예시에서 도출된 Bi-gram으로 보아 ROUGE-2의 Recall 스코어는 다음과 같습니다.

![](/img/in-post/equation6.png)

시스템 요약본은 참조 요약본에 속한 5개의 Bi-gram 중 4개의 Bi-gram을 생성해냈습니다. 이는 꽤나 괜찮은 결과네요. 이제 ROUGE-2의 Precision 스코어는 다음과 같습니다.

![](/img/in-post/equation7.png)

Precision 점수는 시스템 요약본 중 67%의 Bi-gram들이 참조 요약본 내 Bi-gram들과 겹친다는 것을 의미하고 있습니다. 이 역시 나쁘지 않은 결과입니다. 우리는 시스템 요약본과 참조 요약본이 각각 길어질수록 겹치는 Bi-gram들의 수가 적어질 것임을 알아야 합니다. 특히, Abstractive Summarization과 같이 원문을 그대로 사용해 요약하지 않는 경우는 그 정도가 더 심할 것이고요.

어떤 이들이 ROUGE-1를 ROUGE-2 보다 자주 사용하는 (혹은 두 지표를 함께 사용) 이유 역시 모델이 생성해 낸 요약본 혹은 번역본의 **유창성**을 보고자 함에 있습니다. ROUGE-2의 기저에는 모델이 참조 요약본의 단어 순서를 보다 더 잘 따라할수록, 모델이 유창하다고 말할 수 있다라는 생각에서 출발하였습니다.

<br>

## 각기 다른 ROUGE 지표들에 대한 간략한 설명
몇 가지 다른 ROUGE 측정 지표들을 간단히 설명드리겠습니다.

- **ROUGE-N**: Unigram, Bigram, Trigram 그리고 그 이상의 중복되는 n-gram을 비교하고자 합니다.
- **ROUGE-L**: [LCS](https://en.wikipedia.org/wiki/Longest_common_subsequence_problem) 기법을 이용해 최장 길이로 매치되는 문자열을 측정합니다. LCS의 장점은 ROUGE-2와 같이 단어들의 연속적인 매칭을 요구하지 않고, **문자열**들의 매칭만을 측정하고자 하기 때문에 문장과 문장 간에 발생할 수 있는 문자열 매칭 결과 역시 반영할 수 있다는 것입니다. 또한 최장 길이로 매칭된 문자열들을 탐색하는 과정에서 가장 많이 발생하는 길이의 n-gram을 찾게 되므로, 미리 n-gram 길이를 설정할 필요가 없다는 점 역시 장점입니다.
- **ROUGE-S**: 특정 Window size가 주어졌을 때, Window size 내에서 발생하는 단어쌍들을 묶어 해당 단어쌍들이 얼마나 중복되게 나타나는 지를 봅니다. 해당 기법은 Skip-gram co-ocurrence 기법이라 부르기도 합니다. 예를 들어, Skip-bigram은 최대 2칸 내 길이 안에 묶일 수 있는 단어들을 쌍으로 묶어, 해당 쌍들이 얼마나 중복되게 나타나는지를 평가하고 합니다. "cat in the hat"이라는 문장이 있다고 해봅시다. 그렇다면 해당 문장에서 발생할 수 있는 Skip-bigram은 "cat in", "cat the", "cat hat", "in the", "in hat", "the hat"이 되게 됩니다.

ROUGE에 대해 더 깊은 이해를 하시길 원하시는 분들은 원 저자의 [논문](https://www.aclweb.org/anthology/W04-1013/)을 참조하시는 것이 좋습니다. 그리고 어떠한 성능 측정 지표를 사용할 것인지는 여러분이 모델 성능을 평가하고자 하는 태스크에 달려있습니다. 만약 여러분이 원문의 내용을 그대로 사용해 요약문을 생성하는 Extractive Summarization 모델의 성능을 평가하고자 하고 모델이 참조하고자 하는 문장과 생성해내는 문장의 길이가 모두 꽤 긴 편에 속할 경우, ROUGE-1과 ROUGE-L을 사용하는 것이 타당할 것입니다. 그게 아니라 모델이 보다 **정확한** 요약문을 만들어내는지를 보고자 한다면 ROUGE-1만 사용하는 것으로 충분할 것입니다 (**불용어 제거**와 단어들의 **Stemming**까지 수행했을 경우에는 특히 더).
