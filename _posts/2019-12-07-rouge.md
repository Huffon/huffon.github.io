---
layout: post
title: "ROUGE: 예시를 통해 ROUGE 성능 지표 이해하기"
subtitle: '텍스트 요약과 기계번역의 성능 평가에 자주 사용되는 ROUGE 스코어에 대해 알아보자'
author: "devfon"
header-style: text
lang: kr
tags:
  - AI
  - NLP
---

_본 글은 Northeastern University의 강의 자료로 작성된 [**What is ROUGE and how it works for evaluation of
summarization tasks?**](http://www.ccs.neu.edu/home/vip/teach/DMcourse/5_topicmodel_summ/notes_slides/What-is-ROUGE.pdf)를 한국어로 옮긴 글입니다. 원문을 살려서 읽고자 하신 분들은 링크를 참조해주세요._


## ROUGE What is ROUGE and how it works for evaluation of summarization tasks?
ROUGE는 모델의 텍스트 요약 성능을 평가하는 지표로 **Recall-Oriented Understudy for Gisting Evaluation**의 준말입니다. 본질적으로 ROUGE는 **텍스트 자동 요약** 및 **기계 번역**의 성능을 평가하는 지표들을 일컫습니다.
본 지표들은 모델이 생성한 요약본 혹은 번역본을 참조본과 대조해
It works by comparing an automatically produced summary or translation against a set of reference summaries (typically human produced). 우리가 다음과 같은 시스템 요약과 참조 요약을 가지고 있다고 가정해봅시다.

시스템 요약 (모델이 생성한 요약):
> the cat was found under the bed

참조 요약 (Gold standard, 대개 사람이 직접 만든 요약본) :
> the cat was under the bed

만약 우리가 개별 단어들에만 집중한다면 모델이 생성해낸 **시스템 요약**과 **참조 요약** 간 겹치는 단어는 6개이다. 그러나 이 6이라는 숫자는 성능 지표(Metric)로 사용하기 적합하지 않은 수이다. 정량적 지표 값을 얻기 위해 우리는 겹치는 단어 개수를 이용해 **Recall**과 **Precision**을 계산할 수 있어야 한다.

<br/>

## ROUGE에서의 Precision과 Recall
ROUGE 관점에서 Precision과 Recall은 단순하게 나타낼 수 있다. Recall은 참조 요약본를 구성하는 단어 중 몇 개의 단어가 시스템 요약본의 단어와 겹치는지를 보는 지표이다. 우리가 개별 단어를 하나의 단어로 사용한다면, Recall은 다음과 같이 계산될 수 있다.

<그림 1>

그리고 위 예제에서의 Recall은 다음과 같다.

<그림 2>

이 점수의 의미는 참조 요약본의 모든 언어가 모델에 의해 시스템 요약본에 등장했다는 것입니다. 언뜻 보기에 해당 점수는 무척 좋아보일 수 있습니다. 그러나, 이 점수가 요약 성능의 모든 것을 말해주지는 않습니다. 만약 모델이 생성한 요약본이 엄청나게 긴 문장으로 출력될 경우, 시스템 요약 문장 내 대부분의 문장이 참조 요약본과 관련 없는 단어를 포함할지라도 참조 요약의 모든 단어를 포함할 가능성이 크게 됩니다.

이러한 문제를 해결하기 위해 Precision이 필요합니다. Precision은 모델이 생생해 낸 시스템 요약 중 얼마나 관련 있는 단어들이 많은지를 보고자 합니다. 그리고 이는 다음과 같은 식을 통해 구할 수 있습니다.

<그림 3>

위 예제에서 보자면 Precision은 모델이 생성해 낸 문장을 구성하는 7개의 단어 중 6개의 단어만 실제 요약과 연관 있는 단어로 볼 수 있을 것입니다. 예제를 다음과 같이 바꿔봅시다.

시스템 요약 2:
> the tiny little cat was found under the big funny bed

이제 Precision은 다음과 같아집니다.

<그림 4>

그렇게 좋은 결과 같아 보이지는 않네요. Precision의 관점에서 보면 모델은 참조 요약과 최대한 정확하게 일치하는 요약을 생성해내야 합니다. 그러나 모델이 생성한 요약 문장에 불필요한 단어가 너무 많기 때문에 좋은 점수를 받을 수 없는 것이지요. 때문에 모델의 성능을 평가하기 위해서는 Precision과 Recall을 모두 계산한 후, F-Measure를 측정하는 것이 바람직합니다.

If your summaries are in some way forced to be concise through some constraints, then you could consider using just the Recall since precision is of less concern in this scenario.

<br/>

## ROUGE-N, ROUGE-S, 그리고 ROUGE-L은 뭔데?
ROUGE-N, ROUGE-S, ROUGE-L은 각각 요약본의 일정 **부분**을 비교하는 것을 의미합니다. 예를 들어, ROUGE-1은 시스템 요약본과 참조 요약본 간 겹치는 Uni-gram의 수를 보고자 합니다. 그리고 ROUGE-2는 시스템 요약본과 참조 요약본 간 겹치는 Bi-gram의 수를 보고자 하는 것이고요. 위의 예시들을 다시 살펴보도록 합시다. 

우리는 현재 ROUGE-2를 이용해 Precision과 Recall 스코어를 계산하고자 하는 상황입니다.

시스템 요약 :
> the cat was found under the bed

참조 요약 :
> the cat was under the bed

시스템 요약 (bi-grams):
> the cat, cat was, was found, found under, under the, the bed

참조 요약 (bi-grams):
> the cat, cat was, was under, under the, the bed


예시에서 도출된 Bi-gram으로 보아 ROUGE-2의 Recall 스코어는 다음과 같습니다.
<그림 5> 

시스템 요약본은 참조 요약본에 속한 5개의 Bi-gram 중 4개의 Bi-gram을 생성해냈습니다. 이는 꽤나 괜찮은 결과네요. 이제 ROUGE-2의 Precision 스코어는 다음과 같습니다.
<그림 6>

Precision 점수는 시스템 요약본 중 67%의 Bi-gram들이 참조 요약본 내 Bi-gram들과 겹친다는 것을 의미하고 있습니다. 이 역시 나쁘지 않은 결과입니다. 우리는 시스템 요약본과 참조 요약본이 각각 길어질수록 겹치는 Bi-gram들의 수가 적어질 것임을 알아야 합니다. 특히, Abstractive Summarization과 같이 원문을 그대로 사용해 요약하지 않는 경우는 그 정도가 더 심할 것이고요.

어떤 이들이 ROUGE-1를 ROUGE-2 보다 자주 사용하는 (혹은 두 지표를 함께 사용) 이유 역시 모델이 생성해 낸 요약본 혹은 번역본의 **유창성**을 보고자 함에 있습니다. ROUGE-2의 기저에는 모델이 참조 요약본의 단어 순서를 보다 더 잘 따라할수록, 모델이 유창하다고 말할 수 있다라는 생각에서 출발하였습니다.

<br>

## 각기 다른 ROUGE 지표들에 대한 간략한 설명
몇 가지 다른 ROUGE 측정 지표들을 간단히 설명드리겠습니다.

- **ROUGE-N**: Unigram, Bigram, Trigram 그리고 그 이상의 중복되는 n-gram을 비교하고자 합니다.
- **ROUGE-L**: [LCS](https://en.wikipedia.org/wiki/Longest_common_subsequence_problem) 기법을 이용해 최장 길이로 매치되는 문자열을 측정합니다. LCS의 장점은 ROUGE-2와 같이 단어들의 연속적인 매칭을 요구하지 않고, **문자열**들의 매칭만을 측정하고자 하기 때문에 문장과 문장 간에 발생할 수 있는 문자열 매칭 결과 역시 반영할 수 있다는 것입니다. 또한 최장 길이로 매칭된 문자열들을 탐색하는 과정에서 가장 많이 발생하는 길이의 n-gram을 찾게 되므로, 미리 n-gram 길이를 설정할 필요가 없다는 점 역시 장점입니다.
- **ROUGE-S**: 특정 Window size가 주어졌을 때, Window size 내에서 발생하는 단어쌍들을 묶어 해당 단어쌍들이 얼마나 중복되게 나타나는 지를 봅니다. 해당 기법은 Skip-gram co-ocurrence 기법이라 부르기도 합니다. 예를 들어, Skip-bigram은 최대 2칸 내 길이 안에 묶일 수 있는 단어들을 쌍으로 묶어, 해당 쌍들이 얼마나 중복되게 나타나는지를 평가하고 합니다. "cat in the hat"이라는 문장이 있다고 해봅시다. 그렇다면 해당 문장에서 발생할 수 있는 Skip-bigram은 "cat in", "cat the", "cat hat", "in the", "in hat", "the hat"이 되게 됩니다.

ROUGE에 대해 더 깊은 이해를 하시길 원하시는 분들은 원 저자의 [논문](https://www.aclweb.org/anthology/W04-1013/)을 참조하시는 것이 좋습니다. 그리고 어떠한 성능 측정 지표를 사용할 것인지는 여러분이 모델 성능을 평가하고자 하는 태스크에 달려있습니다. 만약 여러분이 원문의 내용을 그대로 사용해 요약문을 생성하는 Extractive Summarization 모델의 성능을 평가하고자 하고 모델이 참조하고자 하는 문장과 생성해내는 문장의 길이가 모두 꽤 긴 편에 속할 경우, ROUGE-1과 ROUGE-L을 사용하는 것이 타당할 것입니다. 그게 아니라 모델이 보다 **정확한** 요약문을 만들어내는지를 보고자 한다면 ROUGE-1만 사용하는 것으로 충분할 것입니다 (**불용어 제거**와 단어들의 **Stemming**까지 수행했을 경우에는 특히 더).
