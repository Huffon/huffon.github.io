---
layout: post
title: "TORCHNLP.SAMPLERS 패키지 파훼하기"
subtitle: '데이터 샘플링에 활용되는 Sampler 예제 학습하기'
author: "devfon"
header-style: text
lang: kr
tags:
  - AI
  - NLP
  - Deep Learning
---

**PyTorch**의 서드 파티인 `torchnlp` 라이브러리에는 데이터 샘플링에 활용되는 다양한 **샘플러** 클래스를 지원하고 있습니다. **샘플러**는 데이터셋으로부터 각각의 샘플들을 어떠한 방식으로 내줄 것인지에 대해 정의하는 클래스입니다. 공식 문서에서 소개하고 있는 **샘플러** 클래스를 살펴보며, 각각의 샘플러가 어떠한 기능을 제공해주고 있는지 살펴보도록 하겠습니다.

### RepeatSampler

```python
torchnlp.samplers.RepeatSampler(sampler)
```
Sampler that repeats forever.

Background:
The repeat sampler can be used with the DataLoader with option to re-use worker processes. Learn more here: https://github.com/pytorch/pytorch/issues/15849
Parameters:	sampler (torch.data.utils.sampler.Sampler) –

<br/>

### SortedSampler

```
torchnlp.samplers.SortedSampler(data, sort_key=<function identity>)
```

**키 함수에 의해 정렬**된 리스트를 항상 **동일한 순서**의 시퀀스 원소로 샘플링해줍니다.

**인자:**
- `data (iterable)`: 이터러블 데이터
- `sort_key (callable)`: 리스트 내 원소를 정렬할 기준이 될 수 있는 키 함수

```python
>>> list(SortedSampler(range(10), sort_key=lambda i: -i))
[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]
```

<br/>

### BalancedSampler

```
torchnlp.samplers.BalancedSampler(data_source, get_class=<function identity>, get_weight=<function BalancedSampler.<lambda>>, **kwargs)
```

**데이터 인스턴스의 클래스**에 따라 **가중치를 적용한 샘플링**을 해줍니다.

**인자:**
- `data (iterable)`: 이터러블 데이터
- `get_class (callable, optional)`: 각 원소의 클래스를 반환해주는 함수
- `get_weight (callable, optional)`: 각 원소에 적용할 가중치 함수
- `kwargs`: [`WeightedRandomSampler`](https://github.com/PetrochukM/PyTorch-NLP/blob/60112df3132b9b45c03ea91bc97dfce5f5bb6ecb/torchnlp/_third_party/weighted_random_sampler.py#L7)에 활용될 추가 키워드 인자 _e.g.) `num_samples`_

```python
>>> from torchnlp.samplers import BalancedSampler, DeterministicSampler
>>> data = ['a', 'b', 'c'] + ['c'] * 100
>>> sampler = BalancedSampler(data, num_samples=3)
>>> sampler = DeterministicSampler(sampler, random_seed=12)
>>> [data[i] for i in sampler]
['c', 'b', 'a']
```

위 예를 보시면 일반적인 샘플링에서는 `c`가 압도적으로 많이 나와야 하지만, `BalancedSampler`에서는 **각 클래스 별 인스턴스**를 활용한 **개별 가중치**가 계산된 후, **전체 클래스 수**를 활용해 `return iter(torch.multinomial(self.weights, self.num_samples, self.replacement).tolist())` 와 같이 [**torch.multinomial**](https://pytorch.org/docs/master/generated/torch.multinomial.html)이 적용되므로 다항 분포에서 샘플링 된 원소들의 인덱스가 반환됩니다. 아래는 개별 가중치가 계산되는 로직입니다.

```python
# 각 인스턴스의 클래스를 기록한 리스트 저장
classified = [get_class(item) for item in data_source]  # [a, b, c, c, c, ...]

# 각 인스턴스의 가중치 함수 적용: 디폴트는 1 반환
weighted = [float(get_weight(item)) for item in data_source]  # [1, 1, 1, 1, ...]

# 두 리스트를 돌며, 각 클래스 별 가중치의 합 계산
class_totals = {
	k: sum([w for c, w in zip(classified, weighted) if k == c]) for k in set(classified)
}  # {a: 1, b: 1, c: 101}

# 가중치 합을 활용해 개별 가중치 리스트 저장
weights = [w / class_totals[c] if w > 0 else 0.0 for c, w in zip(classified, weighted)]  # [0, 0, 0.009, 0.009, ...]
```

<br/>


### DistributedSampler

```python
torchnlp.samplers.DistributedSampler(iterable, num_replicas=None, rank=None)
```

Iterable wrapper that distributes data across multiple workers.

Parameters:	
iterable (iterable) –
num_replicas (int, optional) – Number of processes participating in distributed training.
rank (int, optional) – Rank of the current process within num_replicas.
Example

```python
>>> list(DistributedSampler(range(10), num_replicas=2, rank=0))
[0, 2, 4, 6, 8]
>>> list(DistributedSampler(range(10), num_replicas=2, rank=1))
[1, 3, 5, 7, 9]
```

<br/>

### DistributedBatchSampler

```python
classtorchnlp.samplers.DistributedBatchSampler(batch_sampler, **kwargs)
```

BatchSampler wrapper that distributes across each batch multiple workers.

Parameters:	
batch_sampler (torch.utils.data.sampler.BatchSampler) –
num_replicas (int, optional) – Number of processes participating in distributed training.
rank (int, optional) – Rank of the current process within num_replicas.

```python
>>> from torch.utils.data.sampler import BatchSampler
>>> from torch.utils.data.sampler import SequentialSampler
>>> sampler = SequentialSampler(list(range(12)))
>>> batch_sampler = BatchSampler(sampler, batch_size=4, drop_last=False)
>>>
>>> list(DistributedBatchSampler(batch_sampler, num_replicas=2, rank=0))
[[0, 2], [4, 6], [8, 10]]
>>> list(DistributedBatchSampler(batch_sampler, num_replicas=2, rank=1))
[[1, 3], [5, 7], [9, 11]]
```

<br/>

### BPTTSampler

```python
torchnlp.samplers.BPTTSampler(data, bptt_length, type_='source')
```

Samples sequentially source and target slices of size bptt_length.

Typically, such a sampler, is used for language modeling training with backpropagation through time (BPTT).

Reference: https://github.com/pytorch/examples/blob/c66593f1699ece14a4a2f4d314f1afb03c6793d9/word_language_model/main.py#L122

Parameters:	
data (iterable) – Iterable data.
bptt_length (int) – Length of the slice.
type (str, optional) – Type of slice [‘source’|’target’] to load where a target slice is one timestep ahead
Example

```python
>>> from torchnlp.samplers import BPTTSampler
>>> list(BPTTSampler(range(5), 2))
[slice(0, 2, None), slice(2, 4, None)]
```

<br/>

### BPTTBatchSampler

```python
torchnlp.samplers.BPTTBatchSampler(data, bptt_length, batch_size, drop_last, type_='source')
```
Samples sequentially a batch of source and target slices of size bptt_length.

Typically, such a sampler, is used for language modeling training with backpropagation through time (BPTT).

Reference: https://github.com/pytorch/examples/blob/c66593f1699ece14a4a2f4d314f1afb03c6793d9/word_language_model/main.py#L61

Parameters:	
data (iterable) –
bptt_length (int) – Length of the slice.
batch_size (int) – Size of mini-batch.
drop_last (bool) – If True, the sampler will drop the last batch if its size would be less than batch_size.
type (str, optional) – Type of batch [‘source’|’target’] to load where a target batch is one timestep ahead.
Example

```python
>>> sampler = BPTTBatchSampler(range(100), bptt_length=2, batch_size=3, drop_last=False)
>>> list(sampler)[0] # First Batch
[slice(0, 2, None), slice(34, 36, None), slice(67, 69, None)]
```

<br/>


### BucketBatchSampler

```python
torchnlp.samplers.BucketBatchSampler(sampler, batch_size, drop_last, sort_key=<function identity>, bucket_size_multiplier=100)
```
BucketBatchSampler toggles between sampler batches and sorted batches.

Typically, the sampler will be a RandomSampler allowing the user to toggle between random batches and sorted batches. A larger bucket_size_multiplier is more sorted and vice versa.

Background:
BucketBatchSampler is similar to a BucketIterator found in popular libraries like AllenNLP and torchtext. A BucketIterator pools together examples with a similar size length to reduce the padding required for each batch while maintaining some noise through bucketing.

AllenNLP Implementation: https://github.com/allenai/allennlp/blob/master/allennlp/data/iterators/bucket_iterator.py

torchtext Implementation: https://github.com/pytorch/text/blob/master/torchtext/data/iterator.py#L225

Parameters:	
sampler (torch.data.utils.sampler.Sampler) –
batch_size (int) – Size of mini-batch.
drop_last (bool) – If True the sampler will drop the last batch if its size would be less than batch_size.
sort_key (callable, optional) – Callable to specify a comparison key for sorting.
bucket_size_multiplier (int, optional) – Buckets are of size batch_size * bucket_size_multiplier.
Example

```python
>>> from torchnlp.random import set_seed
>>> set_seed(123)
>>>
>>> from torch.utils.data.sampler import SequentialSampler
>>> sampler = SequentialSampler(list(range(10)))
>>> list(BucketBatchSampler(sampler, batch_size=3, drop_last=False))
[[6, 7, 8], [0, 1, 2], [3, 4, 5], [9]]
>>> list(BucketBatchSampler(sampler, batch_size=3, drop_last=True))
[[0, 1, 2], [3, 4, 5], [6, 7, 8]]
```

<br/>

### DeterministicSampler

```python
torchnlp.samplers.DeterministicSampler(sampler, random_seed, cuda=False)
```

Maintains a random state such that sampler returns the same output every process.

Parameters:	
sampler (torch.data.utils.sampler.Sampler) –
random_seed (int) –
cuda (bool, optional) – If True this sampler forks the random state of CUDA as well.

<br/>

### NoisySortedSampler

```python
classtorchnlp.samplers.NoisySortedSampler(data, sort_key=<function identity>, get_noise=<function _uniform_noise>)[source]
```
Samples elements sequentially with noise.

Background

NoisySortedSampler is similar to a BucketIterator found in popular libraries like AllenNLP and torchtext. A BucketIterator pools together examples with a similar size length to reduce the padding required for each batch. BucketIterator also includes the ability to add noise to the pooling.

AllenNLP Implementation: https://github.com/allenai/allennlp/blob/e125a490b71b21e914af01e70e9b00b165d64dcd/allennlp/data/iterators/bucket_iterator.py

torchtext Implementation: https://github.com/pytorch/text/blob/master/torchtext/data/iterator.py#L225

Parameters:	
data (iterable) – Data to sample from.
sort_key (callable) – Specifies a function of one argument that is used to extract a numerical comparison key from each list element.
get_noise (callable) – Noise added to each numerical sort_key.

```python
>>> from torchnlp.random import set_seed
>>> set_seed(123)
>>>
>>> import random
>>> get_noise = lambda i: round(random.uniform(-1, 1))
>>> list(NoisySortedSampler(range(10), sort_key=lambda i: i, get_noise=get_noise))
[0, 1, 2, 3, 5, 4, 6, 7, 9, 8]
```

<br/>

### OomBatchSampler

```python
torchnlp.samplers.OomBatchSampler(batch_sampler, get_item_size, num_batches=5)
```

Out-of-memory (OOM) batch sampler wraps batch_sampler to sample the num_batches largest batches first in attempt to cause any potential OOM errors early.

Credits: https://github.com/allenai/allennlp/blob/3d100d31cc8d87efcf95c0b8d162bfce55c64926/allennlp/data/iterators/bucket_iterator.py#L43

Parameters:	
batch_sampler (torch.utils.data.sampler.BatchSampler) –
get_item_size (callable) – Measure the size of an item given it’s index int.
num_batches (int, optional) – The number of the large batches to move to the beginning of the iteration.
