---
layout: post
title: "TORCHNLP.SAMPLERS 패키지 파훼하기"
subtitle: '데이터 샘플링에 활용되는 Sampler 예제 학습하기'
author: "devfon"
header-style: text
lang: kr
tags:
  - AI
  - NLP
  - Deep Learning
---

**PyTorch**의 서드 파티인 `torchnlp` 라이브러리에는 데이터 샘플링에 활용되는 다양한 **샘플러** 클래스를 지원하고 있습니다. **샘플러**는 데이터셋으로부터 각각의 샘플들을 어떠한 방식으로 내줄 것인지에 대해 정의하는 클래스입니다. 공식 문서에서 소개하고 있는 **샘플러** 클래스를 살펴보며, 각각의 샘플러가 어떠한 기능을 제공해주고 있는지 살펴보도록 하겠습니다.

### RepeatSampler

```
torchnlp.samplers.RepeatSampler(sampler)
```

Sampler that repeats forever.

The repeat sampler can be used with the DataLoader with option to re-use worker processes. Learn more here: https://github.com/pytorch/pytorch/issues/15849

**인자:**
- `sampler (torch.data.utils.sampler.Sampler)`: PyTorch Sampler 클래스

<br/>

### SortedSampler

```
torchnlp.samplers.SortedSampler(data, sort_key=<function identity>)
```

**키 함수에 의해 정렬**된 리스트를 항상 **동일한 순서**의 시퀀스 원소로 샘플링해줍니다.

**인자:**
- `data (iterable)`: 이터러블 데이터
- `sort_key (callable)`: 리스트 내 원소를 정렬할 기준이 될 수 있는 키 함수

```python
>>> list(SortedSampler(range(10), sort_key=lambda i: -i))
[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]
```

<br/>

### DeterministicSampler

```
torchnlp.samplers.DeterministicSampler(sampler, random_seed, cuda=False)
```

샘플러의 **랜덤 스테이트**를 유지해 매번 동일한 결과 값을 반환하도록 합니다.

**인자:**
- `sampler (torch.data.utils.sampler.Sampler)`: PyTorch Sampler 클래스
- `random_seed (int)`: 랜덤 시드

<br/>

### BalancedSampler

```
torchnlp.samplers.BalancedSampler(data_source, get_class=<function identity>, get_weight=<function BalancedSampler.<lambda>>, **kwargs)
```

**데이터 인스턴스의 클래스**에 따라 **가중치를 적용한 샘플링**을 해줍니다.

**인자:**
- `data (iterable)`: 이터러블 데이터
- `get_class (callable, optional)`: 각 원소의 클래스를 반환해주는 함수
- `get_weight (callable, optional)`: 각 원소에 적용할 가중치 함수
- `kwargs`: [`WeightedRandomSampler`](https://github.com/PetrochukM/PyTorch-NLP/blob/60112df3132b9b45c03ea91bc97dfce5f5bb6ecb/torchnlp/_third_party/weighted_random_sampler.py#L7)에 활용될 추가 키워드 인자 _e.g.) `num_samples`_

```python
>>> from torchnlp.samplers import BalancedSampler, DeterministicSampler
>>> data = ['a', 'b', 'c'] + ['c'] * 100
>>> sampler = BalancedSampler(data, num_samples=3)
>>> sampler = DeterministicSampler(sampler, random_seed=12)
>>> [data[i] for i in sampler]
['c', 'b', 'a']
```

위 예를 보시면 일반적인 샘플링에서는 `c`가 압도적으로 많이 나와야 하지만, `BalancedSampler`에서는 **각 클래스 별 인스턴스**를 활용한 **개별 가중치**가 계산된 후, **전체 클래스 수**를 활용해 `return iter(torch.multinomial(self.weights, self.num_samples, self.replacement).tolist())` 와 같이 [**torch.multinomial**](https://pytorch.org/docs/master/generated/torch.multinomial.html)이 적용되므로 다항 분포에서 샘플링 된 원소들의 인덱스가 반환됩니다. 아래는 개별 가중치가 계산되는 로직입니다.

```python
# 각 인스턴스의 클래스를 기록한 리스트 저장
classified = [get_class(item) for item in data_source]  # [a, b, c, c, c, ...]

# 각 인스턴스의 가중치 함수 적용: 디폴트는 1 반환
weighted = [float(get_weight(item)) for item in data_source]  # [1, 1, 1, 1, ...]

# 두 리스트를 돌며, 각 클래스 별 가중치의 합 계산
class_totals = {
	k: sum([w for c, w in zip(classified, weighted) if k == c]) for k in set(classified)
}  # {a: 1, b: 1, c: 101}

# 가중치 합을 활용해 개별 가중치 리스트 저장
weights = [w / class_totals[c] if w > 0 else 0.0 for c, w in zip(classified, weighted)]  # [0, 0, 0.009, 0.009, ...]
```

<br/>


### DistributedSampler

```
torchnlp.samplers.DistributedSampler(iterable, num_replicas=None, rank=None)
```

여러 개의 `worker`에 걸쳐 사용될 수 있도록 이터러블을 **배분**해줍니다.

**인자:**
- `iterable (iterable)`: 이터러블 데이터
- `num_replicas (int, optional)`: 병렬 훈련에 사용될 프로세스의 수
- `rank (int, optional)`: 현재 프로세스의 랭크 _cf) `num_replicas`보다 작아야 함_

```python
>>> list(DistributedSampler(range(10), num_replicas=2, rank=0))
[0, 2, 4, 6, 8]
>>> list(DistributedSampler(range(10), num_replicas=2, rank=1))
[1, 3, 5, 7, 9]
```

<br/>

### DistributedBatchSampler

```
torchnlp.samplers.DistributedBatchSampler(batch_sampler, **kwargs)
```

여러 개의 `worker`에 걸쳐 사용될 수 있도록 **BatchSampler**를 **배분**해줍니다.

**인자:**
- `batch_sampler (torch.utils.data.sampler.BatchSampler)`: : PyTorch BatchSampler 클래스
- `num_replicas (int, optional)`: 병렬 훈련에 사용될 프로세스의 수
- `rank (int, optional)`: 현재 프로세스의 랭크 _cf) `num_replicas`보다 작아야 함_

```python
>>> from torchnlp.samplers import DistributedBatchSampler
>>> from torch.utils.data.sampler import SequentialSampler, BatchSampler
>>> sampler = SequentialSampler(list(range(12)))
>>> batch_sampler = BatchSampler(sampler, batch_size=4, drop_last=False)
>>> list(batch_sampler)
[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]]
>>> list(DistributedBatchSampler(batch_sampler, num_replicas=2, rank=0))
[[0, 2], [4, 6], [8, 10]]
>>> list(DistributedBatchSampler(batch_sampler, num_replicas=2, rank=1))
[[1, 3], [5, 7], [9, 11]]
```

<br/>

### BPTTSampler

```
torchnlp.samplers.BPTTSampler(data, bptt_length, type_='source')
```

Samples sequentially source and target slices of size bptt_length.

Typically, such a sampler, is used for language modeling training with backpropagation through time (BPTT).

Reference: https://github.com/pytorch/examples/blob/c66593f1699ece14a4a2f4d314f1afb03c6793d9/word_language_model/main.py#L122

**인자:**
- `data (iterable)`: 이터러블 데이터
- `bptt_length (int)`: 슬라이스 길이
- `type (str, optional)`: 슬라이스 타입 ([source|target]) _cf. 타겟 슬라이스는 `right_shited`를 고려해야 함으로 존재하는 옵션_

```python
>>> from torchnlp.samplers import BPTTSampler
>>> list(BPTTSampler(range(5), 2))
[slice(0, 2, None), slice(2, 4, None)]
```

<br/>

### BPTTBatchSampler

```
torchnlp.samplers.BPTTBatchSampler(data, bptt_length, batch_size, drop_last, type_='source')
```

Samples sequentially a batch of source and target slices of size bptt_length.

Typically, such a sampler, is used for language modeling training with backpropagation through time (BPTT).

Reference: https://github.com/pytorch/examples/blob/c66593f1699ece14a4a2f4d314f1afb03c6793d9/word_language_model/main.py#L61

**인자:**
- `data (iterable)`: 이터러블 데이터
- `bptt_length (int)`: 슬라이스 길이
- `batch_size (int)`: 미니 배치의 사이즈
- `drop_last (bool)`: 참이라면, 마지막에 위치하게 될 Non-Full 배치를 활용하지 않음
- `type (str, optional)`: 슬라이스 타입 ([source|target]) _cf. 타겟 슬라이스는 `right_shited`를 고려해야 함으로 존재하는 옵션_

```python
>>> from torchnlp.samplers import BPTTBatchSampler
>>> sampler = BPTTBatchSampler(range(100), bptt_length=2, batch_size=3, drop_last=False)
>>> list(sampler)[0]  # First Batch
[slice(0, 2, None), slice(34, 36, None), slice(67, 69, None)]
```

<br/>


### BucketBatchSampler

```
torchnlp.samplers.BucketBatchSampler(sampler, batch_size, drop_last, sort_key=<function identity>, bucket_size_multiplier=100)
```

BucketBatchSampler toggles between sampler batches and sorted batches.

Typically, the sampler will be a RandomSampler allowing the user to toggle between random batches and sorted batches. A larger bucket_size_multiplier is more sorted and vice versa.

BucketBatchSampler is similar to a BucketIterator found in popular libraries like AllenNLP and torchtext. A BucketIterator pools together examples with a similar size length to reduce the padding required for each batch while maintaining some noise through bucketing.

[AllenNLP 구현체](https://github.com/allenai/allennlp/blob/master/allennlp/data/iterators/bucket_iterator.py), [torchtext 구현체](https://github.com/pytorch/text/blob/master/torchtext/data/iterator.py#L225)

**인자:**
- `sampler (torch.data.utils.sampler.Sampler)`: PyTorch Sampler 클래스
- `batch_size (int)`: 미니 배치의 사이즈
- `drop_last (bool)`: 참이라면, 마지막에 위치하게 될 Non-Full 배치를 활용하지 않음
- `sort_key (callable, optional)`: 리스트 내 원소를 정렬할 기준이 될 수 있는 키 함수
- `bucket_size_multiplier (int, optional)`: Buckets are of size batch_size * bucket_size_multiplier.

```python
>>> from torchnlp.random import set_seed
>>> set_seed(123)
>>>
>>> from torch.utils.data.sampler import SequentialSampler
>>> sampler = SequentialSampler(list(range(10)))
>>> list(BucketBatchSampler(sampler, batch_size=3, drop_last=False))
[[6, 7, 8], [0, 1, 2], [3, 4, 5], [9]]
>>> list(BucketBatchSampler(sampler, batch_size=3, drop_last=True))
[[0, 1, 2], [3, 4, 5], [6, 7, 8]]
```

<br/>

### NoisySortedSampler

```
torchnlp.samplers.NoisySortedSampler(data, sort_key=<function identity>, get_noise=<function _uniform_noise>)[source]
```

Samples elements sequentially with noise.

NoisySortedSampler is similar to a BucketIterator found in popular libraries like AllenNLP and torchtext. A BucketIterator pools together examples with a similar size length to reduce the padding required for each batch. BucketIterator also includes the ability to add noise to the pooling.

[AllenNLP 구현체](https://github.com/allenai/allennlp/blob/e125a490b71b21e914af01e70e9b00b165d64dcd/allennlp/data/iterators/bucket_iterator.py), [torchtext 구현체](https://github.com/pytorch/text/blob/master/torchtext/data/iterator.py#L225)

**인자:**
- `data (iterable)`: 이터러블 데이터
- `sort_key (callable)`: 리스트 내 원소를 정렬할 기준이 될 수 있는 키 함수
- `get_noise (callable)`: `sort_key`에 적용될 노이즈 함수

```python
>>> import random
>>> from torchnlp.random import set_seed
>>> set_seed(123)
>>> get_noise = lambda i: round(random.uniform(-1, 1))
>>> list(NoisySortedSampler(range(10), sort_key=lambda i: i, get_noise=get_noise))
[0, 1, 2, 3, 5, 4, 6, 7, 9, 8]
```

<br/>

### OomBatchSampler

```
torchnlp.samplers.OomBatchSampler(batch_sampler, get_item_size, num_batches=5)
```

Out-of-memory (OOM) 배치 샘플러 wraps batch_sampler to sample the num_batches largest batches first in attempt to cause any potential OOM errors early.

Credits: https://github.com/allenai/allennlp/blob/3d100d31cc8d87efcf95c0b8d162bfce55c64926/allennlp/data/iterators/bucket_iterator.py#L43

**인자:**
- `batch_sampler (torch.utils.data.sampler.BatchSampler)`
- `get_item_size (callable)` Measure the size of an item given it’s index int.
- `num_batches (int, optional)` The number of the large batches to move to the beginning of the iteration.
