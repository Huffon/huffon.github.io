---
layout: post
title: "자연어 처리 모델의 결과 값 평가하기: BLEU 스코어 피하기"
subtitle: 'BLEU 스코어의 단점과 이를 보완한 메트릭에 대하여'
author: "devfon"
header-style: text
lang: kr
tags:
  - AI
  - NLP
  - Deep Learning
---

제가 자연어 처리 분야에 막 입문한 분들에게 자주 받는 질문 중 하나는 출력 값을 텍스트로 내는 모델들의 성능을 어떻게 측정할 수 있느냐에 관한 것입니다. 이처럼 모델이 텍스트를 입력 값으로 넣어 또 다른 텍스트를 출력 값으로 받는 유형의 문제는 **시퀀스-투-시퀀스** 혹은 **문자열 변형 (String transduction)** 문제로 알려져있습니다. 

이러한 문제들은 매우 흥미로운 문제입니다! 시퀀스-투-시퀀스 모델링을 활용하는 태스크들은 주로 아래와 같이 자연어 처리 분야에서 가장 어려운 태스크로 꼽히곤 합니다.

- 텍스트 요약
- 텍스트 간소화
- 질의응답
- 챗봇
- 기계번역

이러한 유형의 기술들은 이제 SF 영화에서만 볼 수 있는 것들이 아닙니다. 수많은 현존하는 어플리케이션들을 보고 있자면, 시퀀스-투-시퀀스 모델링이 왜 이전보다 더 많은 관심을 받고 있는지 알 수 있습니다. 다만 어려운 점은 어떻게 해당 모델들의 성능을 측정할 것이냐에 관한 문제입니다.

이제 막 자연어 처리 분야에 들어선 사람들에게는 불행하게도, 여러분이 모델의 성능을 평가하기 위해 어떠한 메트릭을 사용해야 하는지에 대해 간단한 해답은 없습니다. 또한 시퀀스-투-시퀀스 태스크에 있어 가장 많이 사용되는 메트릭인 BLEU의 경우 큰 단점을 지니고 있습니다. 그리고 이는 BLEU가 애초에 의도한 기계번역이 아닌 다른 태스크에 적용될 때 더 부각되는 단점입니다.

그러나 이 글을 읽고 계신 여러분은 이처럼 잘 정리된 글을 발견하셨으니 행운이라고 하실 수 있겠습니다! 본 포스트에서는 BLEU 메트릭이 어떻게 동작하는지 알아볼 것입니다 (수식은 최소한으로 등장할 것이니 걱정하지 마세요!). 그리고 BLEU의 문제점들을 살펴본 후, 최종적으로 어떻게 이러한 문제를 여러분의 상황에서 해결할 수 있을지에 대해 이야기할 것입니다.

![](/img/in-post/blue.jpg)

<br/>

## 매우 어려운 문제

BLEU는 기게번역 모델의 성능을 측정하기 위해 고안된 메트릭입니다. 따라서 번역 예제를 통해 BLEU를 살펴보도록 합시다. 여기 프랑스어로 작성된 문자가 있습니다.

> J'ai mangé trois filberts.

아래와 같이 영어로 작성된 참조 번역본들도 함께 존재합니다. 몇몇 영어 화자들은 헤이즐넛을 "filberts"라고 지칭하기 때문에, 아래 두 문장은 모두 완벽히 번역된 좋은 문장들입니다.

> I have eaten three hazelnuts.
> I ate three filberts.

이제 신경망 모델이 생성한 번역본을 함께 보시죠.

> I ate three hazelnuts.

신경망 모델이 내놓은 문장으로 인해 매우 어려운 문제가 생겨났습니다. **신경망 모델이 내놓은 번역 문장과 참조 번역본만을 활용해 신경망의 결과 문장이 얼마나 "좋은" 번역인지를 나타내는 하나의 수치 값을 어떻게 정할 수 있을까?**

> **왜 하나의 수치 값만 이용 가능한가요?** 좋은 질문입니다! 만약 우리가 기계번역 시스템을 만들기 위해 머신러닝을 적용한다고 한다면, 우리는 손실 함수에 집어 넣을 수 있는 하나의 실수 스코어가 필요하게 됩니다. 그리고 잠재적인 베스트 스코어를 알고 있다면, 두 스코어 간의 차이를 계산할 수 있습니다. 해당 차이는 

This allows us to give feedback to our system while it’s training―that is, whether a potential change will improve a translation by making the score closer to the ideal score―and to compare trained systems by looking at their scores on the same task.

여러분이 아마 해볼 수 있는 것은 결과 문장의 각 단어들을 살펴본 후, 해당 단어가 참조 번역 문장에 나왔을 경우 1을 나오지 않았을 경우 0을 주는 것입니다. 그리고 카운트에 대한 정규화를 통해 항상 0에서 1 사이의 값을 가지기 위해 여러분은 오버랩 된 단어들을 모델이 생성한 문장의 단어들로 나누어볼 수 있습니다. 해당 결과가 Uni-gram에 대한 Precision 입니다.

우리가 사용하던 예제 "I ate three hazelnuts"로 보자면 모델이 생성해낸 문장 내 모든 단어들이 최소 하나의 참조 번역본 내에 존재하는 것을 확인할 수 있습니다. 따라서 오버랩된 단어의 수 4를 문장 내 전체 단어의 수 4로 나누게 되면 우리는 해당 번역에 대해 1의 스코어를 지니게 됩니다. 아직까지는 좋네요! 하지만 아래 문장을 만나게 되면 어떨까요?

> Three three three three.

위 문장에 대해 동일한 메트릭을 사용해도 우리는 1의 스코어를 지니게 됩니다. 한 눈에 봐도 좋지 않네요... 우리는 모델에게 첫 번째와 같은 번역이 두 번째 번역보다 좋으므로, 첫 번째 번역과 같은 방향으로 훈련을 하고자 한다고 말 할 수 있어야 합니다.

ㅇㅇㅇ

You could tweak the score a bit by capping the number of times to count each word based on the highest number of times it appears in any reference sentence. Using that measure, our first sentence would still get a score of 1, while our second sentence would get a score of only .25.

위 방법은 우리가 "three three three" 문제에서 탈출할 수 있도록 도와줍니다. 그러나 어떠한 이유로 단어들이 알파벳 순으로 정렬되어 있는 문장의 경우 해당 방법이 큰 도움을 줄 수 없게 됩니다. 아래 예를 한 번 봅시다.

> Ate hazelnuts I three

우리가 가진 현재 방법을 활용한다면 위 문장은 1의 스코어를 지니게 됩니다. 가장 좋은 점수이죠.

We can get around this by counting, not individual words, but words that occur next to each other. These are called n-grams, where n is the number of words per group. Unigrams, bigrams, trigrams and 4-grams are made up of chunks of one, two, three and four words respectively.

이 예제에 대해서는 Bigram을 활용해봅시다. 일반적으로 BLEU 스코어는 Unigram, bigram, trigram 그리고 4-gram의 Precision의 평균 값으로 구해지게 됩니다. 그러나 단순하게 생각해보기 위해 잠시 Bigram에 집중해보도록 합시다. 문장의 시작과 끝을 알리는 토큰들도 잠시 배제하고 생각할 것입니다. 이러한 가이드라인 하에 알파벳 순으로 정열된 Bigram은 다음과 같습니다.

> [Ate hazelnuts]
> [hazelnuts I]
> [I three]

만약 우리가 위 Bigram들에 대해 Unigram과 동일한 연산을 취하게 되면 우리는 이제 0의 스코어를 얻게 됩니다. 가장 좋지 않은 점수이죠. "three three three" 예제 역시 .25가 아닌 0의 스코어를 지니게 됩니다. 첫 번째 예문이었던 "I ate three hazelnuts"는 1의 스코어를 지니게 되지만, 안타깝게도 다음 예제도 1의 스코어를 지니게 됩니다.

> I ate.



One way of getting around this is by multiplying the score we have so far by a measure that penalizes sentences that are shorter than any of our reference translations. We can do this by comparing it to the length of the reference sentence that it the closest in length. This is the brevity penalty.

If our output is as long or longer than any reference sentence, the penalty is 1. Since we’re multiplying our score by it, that doesn’t change the final output.

On the other hand, if our output is shorter than any reference sentence, we divide the length of the closest sentence by the length of our output, subtract one from that, and raise e to the power of that whole shebang. Basically, the longer the shortest reference sentence and the shorter our output, the closer the brevity penalty gets to zero.

In our “I ate” example, the output sentence was two words long and the closest reference sentence was four words. This gives us a brevity penalty of 0.36, which, when multiplied by our bi-gram precision score of 1, drops our final score down to just 0.36.

This measure, looking at n-grams overlap between the output and reference translations with a penalty for shorter outputs, is known as BLEU (short for “Bilingual evaluation understudy” which people literally only ever say when explaining the acronym) and was developed by Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu at IBM in 2002. It’s a very popular metric in NLP, particularly for tasks where the output of a system is a text string rather than a classification. This includes machine translation and, increasingly, natural language generation. It’s one solution to the very-hard-problem I proposed back at the beginning of this post: developing a way to assign a single numerical score to a translation that tells us how “good” it is.

It’s also deeply flawed.

<br/>

## BLEU가 지닌 문제점

At this point you may be wondering, “Rachael, if this metric is so flawed, why did you walk us through how to calculate it?” Mainly to show you how reasonable the metric is. It’s fairly intuitive and the underlying idea, that you can evaluate the output of a machine translation system by comparing it to reference translations, has been extremely influential in NLP (although not without its critics).

물론 BLEU 역시 여러 장점을 지니고 있습니다. 
The most relevant ones folks working in NLP are have to do with how convenient it is for researchers.

- BLEU는 모델의 결과 값을 평가할 인간 번역가를 구하는 것보다 빠르고 쉽게 계산이 가능합니다.
- BLEU는 범용적으로 사용될 수 있는 메트릭입니다. 따라서 같은 태스크의 벤치마크와 여러분의 모델을 쉽게 비교할 수 있죠.

불행하게도 이러한 장점 때문에 사람들이 BLEU를 남용하게 되었고, 심지어 BLEU가 합당한 메트릭이 아닌 태스크에도 적용되는 수준에 이르렀습니다.

> BLEU는 사실 제가 제시한 한 문장의 예제가 아닌 코퍼스 레벨의 측정을 위해 고안된 메트릭입니다.
Taking the BLEU score of each sentence in the corpus and then averaging across them will artificially inflate your score and you’ll definitely get dinged by reviewers if you try to publish work where you do it.

BLEU가 남용되지 않고 있다고 하더라도, 여러분이 모델의 BLEU 스코어를 올리기 위해 시간과 돈을 들이기 전에 반드시 아셔야 할 BLEU의 심각한 한계들이 존재합니다. BLEU의 한계에 대해서는 많은 이야기가 오고 가지만, 제가 생각하는 가장 심각한 4가지 한계는 다음과 같습니다.

- BLEU는 의미를 고려하지 않습니다.
- BLEU는 문장 구조를 직접적으로 고려하지 않습니다.
- BLEU는 형태소가 풍부한 언어를 잘 다루지 못합니다.
- BLEU는 인간의 판단을 잘 담아내지 못합니다.

이제 각 단점들을 하나 하나 살펴보며 왜 제가 이러한 것들이 문제라고 생각하는지를 보여드리겠습니다.

### BLEU는 의미를 고려하지 않는다

제게는 의미를 고려하지 않는다는 단점이 기계번역 모델을 평가하기 위해 BLEU 스코어에만 의존하지 않는 가장 큰 이유가 되었습니다. 기계번역 모델의 사용자로서 제 가장 주요한 목적은 오리지널 언어로 작성된 텍스트에 내포된 의미를 이해하는 것입니다. 만약 모델이 내놓은 번역문이 원문의 의미를 그대로 담아내고 있다면, 해당 문장이 구조적으로 혹은 문법적으로 어색함이 있더라도 기쁘게 해당 서비스를 사용할 것입니다.

그러나 BLEU는 의미를 고려하지 않습니다.

It only rewards systems for n-grams that have exact matches in the reference system. That means that a difference in a function word (like “an” or “on”) is penalized as heavily as a difference in a more important content word. It also means that a translation that had a perfectly valid synonym that just didn’t happen to show up in the reference translation will be penalized.

예제를 살펴보며 왜 해당 문제가 심각한지를 보여드리겠습니다.

> 원문 (프랑스어): J’ai mangé la pomme.

> 참조 번역: I ate the apple.

BLEU 스코어를 적용한다면 아래 세 문장은 모두 동일하게 좋지 않은 성능을 보일 것입니다.

> I consumed the apple.
> I ate an apple.
> I ate the potato.

기계번역 시스템의 사용자로의 입장에서 생각해봤을 때, 저는 상위 두 문장은 만족스러운 번역이라 생각할 것 같습니다. 해당 문장들이 참조 번역본과 완전히 동일하지는 않지만, 의미적으로 좋은 번역이 되었기 때문입니다. 반면 세 번째 문장은 완전히 받아드리기 어려운 수준입니다. 원문의 의미를 변경한 번역이기 때문입니다.


One of the metrics based on BLEU, NIST, sort of gets around this by weighting the penalties for mis-matched n-grams. So a mismatch on a more common n-gram (like “of the”) will receive a lower penalty, while a mismatch on a rarer n-gram (like “buffalo buffalo”) will be more highly penalized. But while this solves the problem of giving function words too much weight, it actually makes the problem of penalizing synonyms (like “ambled” for “walked”) worse because those synonyms only show up in rarer r-grams and are therefore assigned a higher penalty.

### BLEU는 문장의 구조를 고려하지 않는다

Perhaps you’re not convinced by the whole “you can still get pretty good BLEU scores even if you’ve messed up a few key words that entirely change the meaning of the sentence” thing. Perhaps some syntax will convince you?

> Syntax is the study of the structure of sentences. It’s the field of study that allows us to formally model sentences like “I saw the dog with the telescope”, which can mean either that I was using the telescope to look at the dog or that the dog had the telescope. The difference between the two meanings can only be captured by modelling the fact that the words in the sentences can have different relationships to each other.

I’m not the world’s greatest syntactican (by a long shot), but even I know that there’s a lot of important internal syntactic structure in natural language, and if you randomly shuffle the order of words in a sentence you either get 1) meaningless word stew or 2) something with a very different meaning.

Fortunately, there’s been a huge amount of work done in developing systems to automate modelling that structure, which is known as parsing.

Unfortunately, BLEU doesn’t build on any of this research. I can understand why you might want to avoid it, since parsing tends to be fairly computationally intensive, and having to parse all your output everytime you evaluate does add some overhead. (Although there are metrics, like the STM, or subtree metric, that do directly compare the parses for the reference and output translations.)

However, the result of not looking at syntactic structure means that outputs that have a completely bonkers surface word order can receive the same score as those that are much more coherent.

There’s a nice illustration of this in Callison-Burch et al (2006). For this set of reference sentences:

> Orejuela appeared calm as he was led to the American plane which will take him to Miami, Florida.
> Orejuela appeared calm while being escorted to the plane that would take him to Miami, Florida.
> Orejuela appeared calm as he was being led to the American plane that was to carry him to Miami in Florida.
> Orejuela seemed quite calm as he was being led to the American plane that would take him to Miami in Florida.

They generated this machine translation.

> Appeared calm when he was taken to the American plane, which will to Miami, Florida.

It’s not a perfect translation — the person’s name is dropped and there’s no verb after “will” in the second half of the sentence — but it’s also not complete nonsense. This example, however, is:

> which will he was, when taken appeared calm to the American plane to Miami, Florida.

The kicker? Both the first and second outputs get the exact same BLEU score even through the first is clearly a better English translation.

### BLEU는 형태소가 풍부한 언어들을 잘 다루지 못한다.

If, like the majority of people on Earth, you happen to use a language other than English, you may have already spotted a problem with this metric: it’s based on word-level matches. For languages with a lot of morphological richness that quickly becomes a problem.

> Morphemes are the smallest unit of meaning in a language, which are combined together to form words. One example in English would be the “s” in “cats” that tells us that there’s more than one cat. Some languages, like Turkish, have a lot of morphemes in a single word while others, like English, generally have fewer morphemes per word.

Consider the following sentences in Shipibo, a language spoken in Peru. (These examples are from “Evidentiality in Shipibo-Konibo, with a comparative overview of the category in Panoan” by Pilar Valenzuela.)

> Jawen jemara ani iki.
> Jawen jemaronki ani iki.

These are both perfectly acceptable translations of the English sentence “her village is large.” You may notice that the middle word, that starts with “jemar-,” has a different ending in the two sentences. The different endings are different morphemes that indicate how certain the speaker is about the fact that the village is large; the top one means they’ve actually been there and the bottom that they heard it was large from someone else.

This particular type of morpheme is known as an “evidentiality marker,” and English doesn’t have them. In Shipibo, however, you need one of these two morphemes for a sentence to be grammatical, so our reference translations would definitely have one of the two. But if we didn’t happen to generate the exact form of the word we had in our reference sentence, BLEU would penalize it for it… even though both sentences capture the English meaning perfectly well.

### BLEU는 인간의 판단을 잘 매핑하지 못한다.

If your eyes started to glaze over when I got into the grammar bits, now’s the point to tune back in.

What’s the final goal of building a machine translation, or chatbot, or question-answering system? You eventually want people to use it, right? And people won’t use a system if it doesn’t give them useful output. So it makes sense that the thing that you actually want to be optimizing for is how good a human using your system likes it. Pretty much all the metrics we use are designed to be different ways of approximating that.

When BLEU was first proposed, the authors did actually do some behavioral tests to make sure that the measure correlated to human judgement. (And props to them for doing that!) Unfortunately, as researchers did more experiments comparing BLEU scores and human judgements, they discovered that this correlation isn’t always very strong and that other measures tend to pattern more closely with human judgements depending on the specific task.

Turian et al (2003), for example, found that BLEU had the poorest correlation with human judgments of machine translation out of three measures, with simple F1 having the strongest correlation with human judgements, followed by NIST. Callison-Burch et al (2006) looked at systemes developed for a shared task (like a Kaggle competition for academics, but without prize money) and found that the relative ranking of those systems were very different depending on whether you were looking at a BLEU scores or human evaluator’s judgements. And Sun (2010) compared three different metrics―BLEU, GTM and TER―and again found that BLEU scores were the least closely correlated with human judgements.

다시 말해 만약 여러분이 사용자가 즐길 수 있는 시스템을 만들고자 한다면, 여러분은 단순히 BLEU 스코어를 높이는데에만 집중해서는 안될 것입니다.

### 저만 BLEU 스코어를 비판하는게 아니에요

아마 여러분 중 몇몇은 아직까지 BLEU가 항상 좋은 메트릭
Maybe you’re still not convinced that BLEU isn’t always the right tool for the job. That’s fine; in fact, I applaud your skepticism! However, I’m far from the only NLP practitioner who’s not the biggest fan of the metric. Here are some quick links to peer reviewed papers with more discussion of some of the other drawbacks of BLEU.

#### Peer reviewed papers:

- Reiter (2018) is a meta-review of ACL papers that use both BLEU and human judgments for evaluation, and found that they only patterned together for system level reviews of machine translation systems specifically.
- Sulem et al (2018) recommend not using BLEU for text simplification. They found that BLEU scores don’t reflect either grammaticality or meaning preservation very well.
- Novikova et al (2017) show that BLEU, as well as some other commonly-used metrics, don’t map well to human judgements in evaluating NLG (natural language generation) tasks.
- Ananthakrishnan et al (2006) lay out several specific objections to BLEU, and have an in-depth exploration of specific errors in English/Hindi translation that BLEU scores well.

And here are some non-peer-reviewed resources. (While they’re probably not going to be as convincing to peer reviewers looking at a research paper you’ve written, they might be easier to convince your boss with.)

#### 다른 참조자료:

- Matt Post from Amazon Research has an excellent discussion of the effects of preprocessing on BLEU scores.
- This blog post by Kirti Vashee, who worked in translation, discusses problems with BLEU from the perspective of translators.
- Yoav Goldberg gave a really good talk that included a discussion on why you shouldn’t use BLEU for NLG at the International Conference of Natural Language Generation in 2018. You can find the slides for here (search for “BLEU can be Misleading” to get the relevant slide). In particular, he and his co-authors found that their sentence simplification model achieved a high BLEU score even through it added, removed or repeated information.
- Ana Marasović’s blog post “NLP’s generalization problem, and how researchers are tackling it” discusses how individual metrics, including BLEU, don’t capture models’ ability to handle data that differs from what they were exposed to during training.

<br/>

## 그렇다면 어떤 메트릭을 이용해야 할까요?

The main thing I want you to use in evaluating systems that have text as output is caution, especially when you’re building something that might eventually go into production. It’s really important for NLP practitioners to think about how our work will be applied, and what could go wrong. Consider this Palestinian man who was arrested because Facebook translated a post saying “good morning” as “attack them”! I’m not picking on Facebook in particular, I just want to point out that NLP products can be higher-stakes than we sometimes realize.

Carefully picking which metrics we optimize for is an important part of ensuring that the systems we work on are actually usable. For tasks like machine translation, for example, I personally think penalizing large changes in the meaning is very important.

That said, there are a lot of automatic evaluation metrics that can be alternatives to BLEU. Some of them will work better for different tasks, so it’s worth spending some time evaluating what the best choice for your specifica project is.

먼저 BLEU의 단점을 해결하기 위해 BLEU의 변형판으로 등장한 유명한 두 개의 메트릭을 살펴봅시다.

- NIST는 희귀도에 따라 n-gram에 가중치를 부여합니다. 이는 자주 등장하는 n-gram을 맞추는 것보다, 그렇지 않은 n-gram을 맞추는 것에 조금 더 높은 점수를 부여한다는 것을 의미합니다.
- ROUGE는 BLEU의 개량 버전으로 Precision 보다는 Recall에 집중합니다. 즉, ROUGE는 모델이 생성해 낸 결과에 얼마나 많은 참조 번역본 내 n-gram이 등장했는지를 중요하게 여기는 메트릭입니다.

There are also a large number of methods you can use to evaluate sequence to sequence models that aren’t based on BLEU. Some of them are measures adopted from other areas of NLP of machine learning.

- Perplexity is a measure from information theory more often used for language modelling. It measures how well the learned probability distribution of words matches that of the input text.
- Word error rate, or WER, is a commonly-used metric in speech recognition. It measures the number of substitutions (“an” for “the”), deletions and insertions in the output sequence given a reference input.
- The F-score, also commonly known as F1, is the mean of precision (how many predictions were correct) and recall (how many of the possible correct predictions were made).

Others were developed specifically for sequence to sequence tasks.

- STM, or the subtree metric (which I mentioned above) compares the parses for the reference and output translations and penalizes outputs with different syntactic structures.
- METEOR is similar to BLEU but includes additional steps, like considering synonyms and comparing the stems of words (so that “running” and “runs” would be counted as matches). Also unlike BLEU, it is explicitly designed to use to compare sentences rather than corpora.
- TER, or the translation error rate, measures the number of edits needed to chance the original output translation into an acceptable human-level translation.
- TERp, or TER-plus, is an extension of TER that also considers paraphrases, stemming, and synonyms.
- hLEPOR is a metric designed to be better for morphologically complex languages like Turkish or Czech. Among other factors it considers things like part-of-speech (noun, verb, etc.) that can help capture syntactic information.
- RIBES, like hLEPOR, doesn’t rely on languages having the same qualities as English. It was designed to be more informative for Asian languages―like Japanese and Chinese―and doesn’t arely on word boundaries.
- MEWR, probably the newest metric on the list, is one that I find particularly exciting: it doesn’t require reference translations! (This is great for low resource languages that may not have a large parallel corpus available.) It uses a combination of word and sentence embeddings (which capture some aspects of meaning) and perplexity to score translations.

Of course, I don’t have space here to cover all the automated metrics that researchers have developed here. Do feel free to chime in the comments with some of your favorites and why you like them, though!

<br/>

## So what you’re saying is… it’s complicated?

That’s pretty much the heart of the matter. Language is complex, which means that measuring language automatically is hard. I personally think that developing evaluation metrics for natural languagae generation might currently be the hardest problem in NLP. (There’s actually an upcoming workshop on that exact thing at NAACL 2019, if you’re as interested as I am.)

That said, there is one pretty good method to make sure that your system is actually getting better at doing things that humans like: You can ask actual people what they think. Human evaluation used to be the standard in machine translation and I think there’s still a place for it. Yes, it’s expensive and, yes, it takes longer. But at least for systems that are going into production, I think you should be doing at least one round of system evaluation with human experts.

Before you get to that round, though, you’ll probably need to use at least one automatic evaluation metric. And I would urge you to use BLEU if and only if:

1. 기계번역 태스크를 수행할 때, 그리고
2. 전체 코퍼스에 대해 평가를 진행할때, 그리고
3. BLEU의 한계에 대해 인지를 하고 있고, 이러한 한계를 받아드릴 준비가 되어있을 때

그렇지 않다면, 여러분의 특정 문제에 보다 적합한 메트릭을 찾기를 추천드립니다.
