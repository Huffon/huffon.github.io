---
layout: post
title: "자연어 처리는 법률 시스템을 어떻게 개선시키고 있을까?"
subtitle: '법률 인공지능의 발전사에 대해 알아보자'
author: "devfon"
header-style: text
lang: kr
tags:
  - AI
  - NLP
  - Deep Learning
---

_본 글은 2020년도 ACL에 제출된 논문 [**How Does NLP Benefit Legal System: A Summary of Legal Artificial Intelligence**](https://arxiv.org/abs/2004.12158)를 보고 기록을 위해 정리한 글입니다. 보다 자세한 내용을 참조하기 위해서는 원 논문을 참조해주시기 바랍니다._

## 1. 서론

법률 인공지능(이하 법률 AI)은 법률 태스크를 돕기 위해 인공지능 기술을 적용하기 위한 연구입니다. 법률 시장에서 판결문, 계약서, Legal Opinions 등 대부분의 리소스는 텍스트 형태로 표현됩니다. 따라서 법률 AI 태스크들은 주로 자연어 처리 기술에 의존을 하게 됩니다.

법률 AI는 법률 시장에서 법조인들의 반복적인 업무를 줄여줄 수 있는 아주 중요한 역할을 수행할 수 있습니다. 법조계에 존재하는 많은 태스크들은 법률 도메인 전문가의 지식과 다양한 법률 문서의 철저한 이해를 필요로 합니다. 때문에 필요한 법률 문서를 찾고, 이해하는 일은 법률 전문가에게도 많은 시간을 소요해야 하는 일입니다. 그러므로 잘 설계된 법률 AI는 이러한 반복적인 소모 시간을 줄일 수 있고, 이를 통해 법조계의 발전에 이바지 할 수 있습니다. 게다가 법률 AI는 법률 지식에 친숙하지 않은 사람들에게 신뢰할 만한 자료를 제공하는 형태로 발전할 수도 있으며, 이를 통해 법률적 도움이 필요한 이들에게 보다 저렴한 가격에 서비스를 제공할 수도 있습니다.

법률 AI를 발전시키기 위해 지난 수 십년간 많은 연구자들의 노력이 있어왔습니다. 초창기 연구들은 당시 연산량의 한계 때문에 잘 정비된 규칙 혹은 피쳐 기반의 방식을 활용하였습니다. 그리고 최근 Deep Learning의 빠른 발전으로 인해, 연구자들은 Deep Learning 기법들을 법률 AI에 적용하려는 시도를 하고 있습니다. 또한 법률 AI의 발전을 위해 다양한 벤치마크 데이터셋이 등장하기도 했는데, 이를 통해 "판결 예측", "법률 문서 생성", "법률 엔티티 추출 및 분류", "법률 Q&A", "법률 문서 요약" 등 다양한 태스크가 생겨났습니다.

앞서 언급했듯, 여러 연구자들의 노력을 통해 법률 AI에는 많은 발전이 있었습니다. 이러한 발전사를 간단히 요약하자면 해석 가능한, 잘 정돈된 심볼을 활용해 법률 태스크를 해결하고자 하는 시도가 있는가하면, 신경망 기반의 모델들을 활용해 성능 향상에 초점을 둔 시도들도 존재하였습니다. 더 자세히 이야기하자면, 심볼 기반의 방법론들은 해석 가능한 법률 지식을 활용해 법률 문서 내 심볼 간 관계를 추론하기 위해 사용됩니다. 반면 신경망을 활용한 임베딩 기반 모델들은 큰 규모의 데이터로부터 태스크 수행을 위한 잠재 피처를 학습하기 위해 사용됩니다. 

두 방법론의 차이는 법률 AI의 시도들에 있어 여러 문제를 야기해왔습니다. 심볼 기반의 모델은 효과적이지 않지만 해석 가능합니다. 이에 반해 임베딩 기반의 모델은 좋은 성능을 자랑하지만, 일반적으로 해석하기 어려운 경우가 많고 이는 젠더 편향, 인종 차별 등의 윤리적 문제를 법률 시스템에 가져올 수 있다는 문제가 있습니다. 이러한 개별적인 단점들이 현존하는 모델들을 실세계에 적용하기 어렵게 만들고 있습니다.

다음으로 법률 AI에 있어 임베딩 기반, 심볼 기반 방법론을 막론하고 중요하게 여겨지고 있는 세 가지 사안을 요약해보았습니다. 

1. **지식 모델링**: 법률 문서는 형식적으로 잘 작성된 문서로, 문서 내에 많은 도메인 지식과 개념을 포함하고 있습니다. 따라서 이러한 법률 지식을 활용하는 것은 매우 중요합니다.
2. **법률적 추론**: 자연어 처리 내 많은 태스크들이 추론을 필요로 하지만, 법률 AI 태스크들은 다소 다른 점이 있습니다. 법률적 추론은 법에 정의돈 규칙들을 엄격하게 따라야 한다는 것입니다. 그러므로 법률적 추론을 위해서는 기정의된 규칙과 AI 기술을 잘 혼합하는 것이 중요합니다.
3. **해석 가능성**: 법률 AI에 의해 내려진 결정들은 대개 실제 서비스에 적용되기 위해 해석 가능성을 지녀야 합니다. 그렇지 않다면 공정성에 해가 갈 위험이 있습니다. 따라서 법률 AI에 있어 해석 가능성은 성능만큼이나 중요한 요소입니다.

<br/>

## 2. 임베딩 기반 방법론

먼저 Representation Learning이라고도 불리우는 임베딩 기반의 방법론들에 대해 살펴봅시다. 임베딩 기반 방법론들은 법률적 사실 정보와 지식을 임베딩 공간에 표현하는 것에 주로 관심이 있습니다. 그리고 임베딩 이후, Deep Learning 기반의 방법론들을 활용해 태스크를 수행합니다.

### 2.1 캐릭터, 단어, 개념 임베딩

문자 및 단어 임베딩은 이산적인 텍스트를 연속적인 벡터 공간으로 임베딩하는데 사용되기 때문에 자연어 처리에 있어 핵심적인 역할을 수행합니다. 여러 임베딩 기법들은 그 효과를 증명해왔기 때문에 태스크 수행에 있어 아주 중요합니다.

그러나 법률 문서에 등장한 전문 용어들의 의미를 곧바로 학습하는 것은 현재까지는 불가능에 가깝습니다. 이전까지의 방법들은 대개 Word2Vec과 같은 임베딩 기법을 법률 도메인 코퍼스에 적용하는 것에 초점이 맞추어져 있습니다. 전문 용어를 학습하는 어려움을 해결하기 위해 우리는 문법 정보와 법률 정보를 함께 임베딩시켜 사용해볼 수 있을 것입니다. 지식 모델링은 앞서 언급했듯, 많은 결과들은 법률적 규칙과 지식에 의해 결정되기 때문에 매우 중요한 요소입니다.


지식 그래프 기법이 법률 도메인에서 각광을 받고 있기는 하지만, 지식 그래프를 실용적으로 사용하기에는 두 가지 큰 문제가 있습니다. 첫 번째 문제는 법률 AI에 있어 지식 그래프를 그리는 것이 매우 복잡하다는 것입니다. 대부분의 시나리오에서 법률 지식을 그래프로 미리 그려둔 자료가 없기 때문에, 연구자들은 이를 처음부터 그려야 합니다. 또한 국가마다 다른 법률 시스템과 개념 및 의미를 지니기 때문에 일반적인 법률 지식 그래프를 그리기 조차 쉽지 않습니다. 몇몇 연구자들은 법률 사전을 임베딩하려는 시도를 보이고 있으며, 이는 하나의 대안이 될 수도 있습니다.

두 번째 문제는 법률 지식 그래프가 일반적으로 자연어 처리에 있어 사용되는 지식 그래프와 다른 형태를 지닌다는데에 있습니다. 보통의 지식 그래프는 엔티티와 개념 간 관계에 대해서 다루지만, 법률 AI의 지식 그래프는 법률 개념을 설명하는데 더 초점이 맞추어져 있습니다. 이러한 두 가지 문제는 법률 AI에 있어 임베딩을 통한 지식 모델링이 어려워지게 하고 있으며, 연구자들은 이러한 문제를 해결하기 위한 연구를 수행해볼 수 있을 것입니다.

### 2.2 사전 훈련 언어 모델

BERT와 같은 사전 훈련 언어 모델 (이하 PLM)은 최근 자연어 처리의 많은 분야에 있어 관심을 받고 있습니다. PLM의 성공으로 보아, 법률 AI에 있어서 역시 PLM을 사용하는 것은 매우 합리적 선택입니다. 그러나 PLM을 학습하는데 사용된 코퍼스와 법률 텍스트에는 큰 차이가 존재합니다. 때문에 일반적인 PLM을 바로 법률 태스크에 활용하고자 한다면 엄청나게 실망스러운 결과를 얻게 될 것입니다. 이러한 차이는 법률 텍스트에 내포된 많은 전문 용어와 지식들에서 기인합니다. 그리고 이러한 문제를 해결하기 위해 중국어로 작성된 법률 문서에 언어 모델을 훈련시킨 [PLM](Zhong 2019b)이 고안되기도 하였습니다. 그리고 법률 도메인에 특화된 PLM은 일반 PLM 보다 법률 AI 태스크에 있어 훨씬 뛰어난 성능을 보였습니다.

법률 AI에 있어 PLM을 더 발전시키기 위해 연구자들은 지식 정보를 PLM에 통합할 수도 있을 것입니다. 그리고 이는 법률 개념을 활용한 PLM의 추론 능력에 도움이 됩니다. 다른 일반 도메인의 지식을 모델에 통합하는 많은 연구가 이미 존재하기 때문에, 이러한 연구를 잘 활용한다면 법률 지식도 모델에 잘 학습시킬 수 있을 것입니다.

<br/>

## 3. 심볼 기반 방법론

이번에는 구조화된 예측을 수행하는 심볼 기반의 방법론들을 살펴봅시다. 심볼 기반의 방법론들은 법률 도메인의 심볼과 지식을 활용해 태스크를 수행하고자 합니다. 사건과 관계와 같은 기호적 법률 지식은 모델에 해석 가능성을 부여합니다. 그리고 더 나은 성능을 위해 심볼 기반의 방법론들에 Deep Learning 기법들을 적용해볼 수도 있습니다. 

### 3.1 정보 추출

정보 추출 (이하 IE)은 자연어 처리에 있어 매우 널리 연구된 태스크입니다. IE는 텍스트로부터 가치있는 정보를 추출하기 위해 존재합니다. 그리고 자연어 처리에도 Named Entity Recognition, Relation Extraction 그리고 Event Extraction과 같이 IE에 집중하는 다양한 태스크들이 있습니다.

법률 AI에 있어 IE 역시 많은 연구자들의 관심을 끌었습니다. 법률 문서의 특수성을 활용하기 위해 연구자들은 법률 NER을 위해 온톨로지 등을 활용하였습니다. 또한 법률 문서에서 사건과 관계 등을 추출하기 위해서는 규칙 기반, CRF, SVM, CNN, GRU 등 다양한 자연어 처리 기법을 활용하였습니다. 현존하는 연구들은 IE의 성능을 끌어올리는데 많은 관심을 가져왔지만, 여기서는 모델로부터 추출된 정보에 관심을 기울여봅시다. 추출된 심볼들은 법률적 기반을 가지고 있으며, 때문에 법률 어플리케이션에 해석 가능성을 부여할 수 있습니다. 따라서 단순히 성능에만 집중을 하는 것은 좋지 않습니다.

아래 추출된 법률 심볼들이 법률 AI에 있어 해석 가능성을 어떻게 부여할 수 있는지에 대한 예시를 한 번 살펴봅시다.

<br/>

**관계 추출과 상속 분쟁**

상속 분쟁은 민법 유형의 케이스로 상속권의 분배에 대해 주로 다루게 된다. 따라서 사망한 사람과 더 가까운 관계의 이해 관계자들이 더 많은 유산을 상속 받을 수 있도록 하기 위해, 관계를 파악하고 추출하는 것이 중요해진다. 해당 목표를 위해 상속 분쟁에 있어서의 관계 추출은 판결 결과에 근거를 제공할 수 있으며, 결과적으로 성능 향상에까지 기여할 수 있게 된다.

**사건 타임라인 추출과 형사 사건의 판결 예측**

형사 사건에서, 여러 집단이 종종 집된 범죄에 연루될 수도 있다. 이러한 경우 어떤 사람이 범죄에 가장 큰 책임이 있는지 결정하기 위해, 우리는 사건에 연루된 모든 관계자들이 사건이 진행되는 동안 무엇을 했는지를 파악할 필요가 있다. 그리고 이러한 사건들의 순서 역시 매우 중요하다. 예를 들어, 패싸움의 경우 처음으로 싸움을 시작한 사람이 가장 큰 책임을 물어야 한다. 따라서 잘 설계된 사건 타임라인 추출 모델은 형사 사건의 판결 예측을 위해 필수적이다.

앞으로 우리는 추출된 정보들을 법률 AI에 활용하는 방법에 대해 고민할 필요가 있을 것입니다. 해당 정보들을 어떻게 활용하느냐는 태스크의 요구 사항에 따라 다를 것이며, 어떻게 사용이 되든 이는 해석 가능성을 제공해준다는 점에서 이점이 있습니다.

### 3.2 법률적 요소 추출

In addition to those common symbols in general NLP, LegalAI also has its exclusive symbols, named legal elements. The extraction of legal elements focuses on extracting crucial elements like whether someone is killed or something is stolen. These elements are called constitutive elements of crime, and we can directly convict offenders based on the results of these elements. Utilizing these elements can not only bring intermediate supervision information to the judgment prediction task but also make the prediction results of the model more interpretable.

Towards a more in-depth analysis of elementbased symbols, Shu et al. (2019) propose a dataset for extracting elements from three different kinds of cases, including divorce dispute, labor dispute, and loan dispute. The dataset requires us to detect whether the related elements are satisfied or not, and formalize the task as a multi-label classification problem. To show the performance of existing methods on element extraction, we have conducted experiments on the dataset, and the results can be found in Table 2.

We have implemented several classical encoding models in NLP for element extraction, including TextCNN, DPCNN, LSTM, BiDAF, and BERT. We have tried two different versions of pretrained parameters of BERT, including the origin parameters (BERT) and the parameters pretrained on Chinese legal documents (BERT-MS). From the results, we can see that the language model pretrained on the general domain performs worse than domain-specific PLM, which proves the necessity of PLM in LegalAI. For the following parts of our paper, we will use BERT pretrained on legal documents for better performance.

From the results of element extraction, we can find that existing methods can reach a promising performance on element extraction, but are still not sufficient for corresponding applications. These elements can be regarded as pre-defined legal knowledge and help with downstream tasks. How to improve the performance of element extraction is valuable for further research.

<br/>

## 4. 법률 AI 어플리케이션

In this section, we will describe several typical applications in LegalAI, including Legal Judgment Prediction, Similar Case Matching and Legal Question Answering. Legal Judgment Prediction and Similar Case Matching can be regarded as the core function of judgment in Civil Law and Common Law system, while Legal Question Answering can provide consultancy for those who are unfamiliar with the legal domain. Therefore, exploring these three tasks can cover most aspects of LegalAI.

### 4.1 법률 판단 예측

Legal Judgment Prediction (LJP) is one of the most critical tasks in LegalAI, especially in the Civil Law system. In the Civil Law system, the judgment results are decided according to the facts and the statutory articles. One will receive legal sanctions only after he or she has violated the prohibited acts prescribed by law. The task LJP mainly concerns how to predict the judgment results from both the fact description of a case and the contents of the statutory articles in the Civil Law system.

As a result, LJP is an essential and representative task in countries with Civil Law system like France, Germany, Japan, and China. Besides, LJP has drawn lots of attention from both artificial intelligence researchers and legal professionals. In the following parts, we describe the research progress and explore the future direction of LJP.

**관련 연구**

LJP has a long history. Early works revolve around analyzing existing legal cases in specific circumstances using mathematical or statistical methods. The combination of mathematical methods and legal rules makes the predicted results interpretable.

To promote the progress of LJP, Xiao et al. (2018) have proposed a large-scale Chinese criminal judgment prediction dataset, C-LJP. The dataset contains over 2.68 million legal documents published by the Chinese government, making C-LJP a qualified benchmark for LJP. C-LJP contains three subtasks, including relevant articles, applicable charges, and the term of penalty. The first two can be formalized as multi-label classification tasks, while the last one is a regression task. Besides, English LJP datasets also exist (Chalkidis et al., 2019a), but the size is limited.

With the development of the neural network, many researchers begin to explore LJP using deep learning technology. These works can be divided into two primary directions. The first one is to use more novel models to improve performance. Chen et al. (2019) use the gating mechanism to enhance the performance of predicting the term of penalty. Pan et al. (2019) propose multi-scale attention to handle the cases with multiple defendants. Besides, other researchers explore how to utilize legal knowledge or the properties of LJP. Luo et al. (2017) use the attention mechanism between facts and law articles to help the prediction of applicable charges. Zhong et al. (2018) present a topological graph to utilize the relationship between different tasks of LJP. Besides, Hu et al. (2018) incorporate ten discriminative legal attributes to help predict low-frequency charges.

**실험 및 분석**

To better understand recent advances in LJP, we have conducted a series of experiments on CLJP. Firstly, we implement several classical text classification models, including TextCNN, DPCNN, LSTM, and BERT. For the parameters of BERT, we use the pretrained parameters on Chinese criminal cases (Zhong et al., 2019b). Secondly, we implement several models which are specially designed for LJP, including FactLaw (Luo et al., 2017), TopJudge (Zhong et al., 2018), and Gating Network (Chen et al., 2019). The results can be found in Table 4. 

From the results, we can learn that most models can reach a promising performance in predicting high-frequency charges or articles. However, the models perform not well on low-frequency labels as there is a gap between micro-F1 and macro-F1. Hu et al. (2018) have explored few-shot learning for LJP. However, their model requires additional attribute information labelled manually, which is time-consuming and makes it hard to employ the model in other datasets. Besides, we can find that performance of BERT is not satisfactory, as it does not make much improvement from those models with fewer parameters. The main reason is that the length of the legal text is very long, but the maximum length that BERT can handle is 512. According to statistics, the maximum document length is 56, 694, and the length of 15% documents is over 512. Document understanding and reasoning techniques are required for LJP.

Although embedding-based methods can achieve promising performance, we still need to consider combining symbol-based with embedding-based methods in LJP. Take TopJudge as an example, this model formalizes topological order between the tasks in LJP (symbol-based part) and uses TextCNN for encoding the fact description. By combining symbol-based and embedding-based methods, TopJudge has achieved promising results on LJP. Comparing the results between TextCNN and TopJudge, we can find that just integrating the order of judgments into the model can lead to improvements, which proves the necessity of combining embedding-based and symbol-based methods. 

For better LJP performance, some challenges require the future efforts of researchers: (1) Document understanding and reasoning techniques are required to obtain global information from extremely long legal texts. (2) Few-shot learning. Even low-frequency charges should not be ignored as they are part of legal integrity. Therefore, handling in-frequent labels is essential to LJP. (3) Interpretability. If we want to apply methods to real legal systems, we must understand how they make predictions. However, existing embedding-based methods work as a black box. What factors affected their predictions remain unknown, and this may introduce unfairness and ethical issues like gender bias to the legal systems. Introducing legal symbols and knowledge mentioned before will benefit the interpretability of LJP.

### 4.2 유사 케이스 매칭

In those countries with the Common Law system like the United States, Canada, and India, judicial decisions are made according to similar and representative cases in the past. As a result, how to identify the most similar case is the primary concern in the judgment of the Common Law system.

In order to better predict the judgment results in the Common Law system, Similar Case Matching (SCM) has become an essential topic of LegalAI. SCM concentrate on finding pairs of similar cases, and the definition of similarity can be various. SCM requires to model the relationship between cases from the information of different granularity, like fact level, event level and element level. In other words, SCM is a particular form of semantic matching, which can benefit the legal information retrieval.

**관련 연구**

Traditional methods of Information Retrieve (IR) focus on term-level similarities with statistical models, including TF-IDF and BM25, which are widely applied in current search systems. In addition to these term matching methods, other researchers try to utilize meta-information to capture semantic similarity. Many machine learning methods have also been applied for IR like SVD or factorization. With the rapid development of deep learning technology and NLP, many researchers apply neural models, including multi-layer perceptron, CNN, and RNN to IR.

There are several LegalIR datasets, including COLIEE (Kano et al., 2018), CaseLaw (Locke and Zuccon, 2018), and CM (Xiao et al., 2019). Both COLIEE and CaseLaw are involved in retrieving most relevant articles from a large corpus, while data examples in CM give three legal documents for calculating similarity. These datasets provide benchmarks for the studies of LegalIR. Many researchers focus on building an easy-to-use legal search engine. They also explore utilizing more information, including citations and legal concepts . Towards the goal of calculating similarity in semantic level, deep learning methods have also been applied to LegalIR. Tran et al. (2019) propose a CNN-based model with document and sentence level pooling which achieves the state-of-the-art results on COLIEE, while other researchers explore employing better embedding methods for LegalIR.

**실험 및 분석**

To get a better view of the current progress of LegalIR, we select CM for experiments. CM contains 8, 964 triples where each triple contains three legal documents (A, B, C). The task designed in CM is to determine whether B or C is more similar to A. We have implemented four different types of baselines: (1) Term matching methods, TF-IDF. (2) Siamese Network with two parametershared encoders, including TextCNN, BiDAF and BERT, and a distance function. (3) Semantic matching models in sentence level, ABCNN, and document level, SMASH-RNN. The results can be found in Table 5.

From the results, we observe that existing neural models which are capable of capturing semantic information outperform TF-IDF, but the performance is still not enough for SCM. As Xiao et al. (2019) state, the main reason is that legal professionals think that elements in this dataset define the similarity of legal cases. Legal professionals will emphasize on whether two cases have similar elements. Only considering term-level and semantic-level similarity is insufficient for the task. 

For the further study of SCM, there are two directions which need future effort: (1) Elementalbased  epresentation. Researchers can focus more on symbols of legal documents, as the similarity of legal cases is related to these symbols like elements. (2) Knowledge incorporation. As semantic-level matching is insufficient for SCM, we need to consider about incorporating legal knowledge into models to improve the performance and provide interpretability.

### 4.3 법률 Q&A

Another typical application of LegalAI is Legal Question Answering (LQA) which aims at answering questions in the legal domain. One of the most important parts of legal professionals’ work is to provide reliable and high-quality legal consulting services for non-professionals. However, due to the insufficient number of legal professionals, it is often challenging to ensure that non-professionals can get enough and high-quality consulting services, and LQA is expected to address this issue.

In LQA, the form of questions varies as some questions will emphasize on the explanation of some legal concepts, while others may concern the analysis of specific cases. Besides, questions can also be expressed very differently between professionals and non-professionals, especially when describing domain-specific terms. These problems bring considerable challenges to LQA, and we conduct experiments to demonstrate the difficulties of LQA better in the following parts.

**관련 연구**

In LegalAI, there are many datasets of question answering. Duan et al. (2019) propose CJRC, a legal reading comprehension dataset with the same format as SQUAD 2.0, which includes span extraction, yes/no questions, and unanswerable questions. Besides, COLIEE contains about 500 yes/no questions. Moreover, the bar exam is a professional qualification examination for lawyers, so bar exam datasets may be quite hard as they require professional legal knowledge and skills.

In addition to these datasets, researchers have also worked on lots of methods on LQA. The rulebased systems  are prevalent in early research. In order to reach better performance, researchers utilize more information like the explanation of concepts or formalize relevant documents as graphs to help reasoning . Machine learning and deep learning methods like CRF, SVM, and CNN have also been applied to LQA. However, most existing methods conduct experiments on small datasets, which makes them not necessarily applicable to massive datasets and real scenarios.

**실험 및 분석**

We select JEC-QA as the dataset of the experiments, as it is the largest dataset collected from the bar exam, which guarantees its difficulty. JEC-QA contains 28, 641 multiple-choice and multiple-answer questions, together with 79, 433 relevant articles to help to answer the questions. JEC-QA classifies questions into knowledge-driven questions (KD-Questions) and case-analysis questions (CA-Questions) and reports the performances of humans. We implemented several representative question answering models, including BiDAF, BERT, Co-matching, and HAF (Zhu et al., 2018). The experimental results can be found in Table 6.

From the experimental results, we can learn the models cannot answer the legal questions well compared with their promising results in open-domain question answering and there is still a huge gap between existing models and humans in LQA.

For more qualified LQA methods, there are several significant difficulties to overcome: (1) Legal multi-hop reasoning. As Zhong et al. (2019a) state, existing models can perform inference but not multi-hop reasoning. However, legal cases are very complicated, which cannot be handled by singlestep reasoning. (2) Legal concepts understanding. We can find that almost all models are better at case analyzing than knowledge understanding, which proves that knowledge modelling is still challenging for existing methods. How to model legal knowledge to LQA is essential as legal knowledge is the foundation of LQA.

<br/>

## 5. 결론

In this paper, we describe the development status of various LegalAI tasks and discuss what we can do in the future. In addition to these applications and tasks we have mentioned, there are many other tasks in LegalAI like legal text summarization and information extraction from legal contracts. Nevertheless, no matter what kind application is, we can apply embedding-based methods for better performance, together with symbol-based methods for more interpretability.

Besides, the three main challenges of legal tasks remain to be solved. Knowledge modelling, legal reasoning, and interpretability are the foundations on which LegalAI can reliably serve the legal domain. Some existing methods are trying to solve these problems, but there is still a long way for researchers to go.

In the future, for these existing tasks, researchers can focus on solving the three most pressing challenges of LegalAI combining embedding-based and symbol-based methods. For tasks that do not yet have a dataset or the datasets are not large enough, we can try to build a large-scale and highquality dataset or use few-shot or zero-shot methods to solve these problems.

Furthermore, we need to take the ethical issues of LegalAI seriously. Applying the technology of LegalAI directly to the legal system will bring ethical issues like gender bias and racial discrimination. The results given by these methods cannot convince people. To address this issue, we must note that the goal of LegalAI is not replacing the legal professionals but helping their work. As a result, we should regard the results of the models only as a reference. Otherwise, the legal system will no longer be reliable. 

For example, professionals can spend more time on complex cases and leave the simple cases for the model. However, for safety, these simple cases must still be reviewed. In general, LegalAI should play as a supporting role to help the legal system.
