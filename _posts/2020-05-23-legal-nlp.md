---
layout: post
title: "자연어 처리는 법률 시스템을 어떻게 개선시키고 있을까?"
subtitle: '법률 인공지능의 발전사'
author: "devfon"
header-style: text
lang: kr
tags:
  - AI
  - NLP
  - Deep Learning
---

_본 글은 2020년도 ACL에 제출된 논문 [How Does NLP Benefit Legal System: A Summary of Legal Artificial Intelligence](https://arxiv.org/abs/2004.12158)을 보고 기록을 위해 정리한 글입니다. 보다 자세한 내용을 참조하기 위해서는 원 논문을 참조해주시기 바랍니다._

<br/>

## 1. 서론

법률 인공지능(이하 법률 AI)은 법률 태스크를 돕기 위해 인공지능 기술을 적용하기 위한 연구입니다. 법률 시장에서 판결문, 계약서, Legal Opinions 등 대부분의 리소스는 텍스트 형태로 표현됩니다. 따라서 법률 AI 태스크들은 주로 자연어 처리 기술에 의존을 하게 됩니다.

법률 AI는 법률 시장에서 법조인들의 반복적인 업무를 줄여줄 수 있는 아주 중요한 역할을 수행할 수 있습니다. 법조계에 존재하는 많은 태스크들은 법률 도메인 전문가의 지식과 다양한 법률 문서의 철저한 이해를 필요로 합니다. 때문에 필요한 법률 문서를 찾고, 이해하는 일은 법률 전문가에게도 많은 시간을 소요해야 하는 일입니다. 그러므로 잘 설계된 법률 AI는 이러한 반복적인 소모 시간을 줄일 수 있고, 이를 통해 법조계의 발전에 이바지 할 수 있습니다. 게다가 법률 AI는 법률 지식에 친숙하지 않은 사람들에게 신뢰할 만한 자료를 제공하는 형태로 발전할 수도 있으며, 이를 통해 법률적 도움이 필요한 이들에게 보다 저렴한 가격에 서비스를 제공할 수도 있습니다.

법률 AI를 발전시키기 위해 지난 수 십년간 많은 연구자들의 노력이 있어왔습니다. 초창기 연구들은 당시 연산량의 한계 때문에 잘 정비된 규칙 혹은 피쳐 기반의 방식을 활용하였습니다. 그리고 최근 Deep Learning의 빠른 발전으로 인해, 연구자들은 Deep Learning 기법들을 법률 AI에 적용하려는 시도를 하고 있습니다. 또한 법률 AI의 발전을 위해 다양한 벤치마크 데이터셋이 등장하기도 했는데, 이를 통해 "판결 예측", "법률 문서 생성", "법률 엔티티 추출 및 분류", "법률 Q&A", "법률 문서 요약" 등 다양한 태스크가 생겨났습니다.

앞서 언급했듯, 여러 연구자들의 노력을 통해 법률 AI에는 많은 발전이 있었습니다. 이러한 발전사를 간단히 요약하자면 해석 가능한, 잘 정돈된 심볼을 활용해 법률 태스크를 해결하고자 하는 시도가 있는가하면, 신경망 기반의 모델들을 활용해 성능 향상에 초점을 둔 시도들도 존재하였습니다. 더 자세히 이야기하자면, 심볼 기반의 방법론들은 해석 가능한 법률 지식을 활용해 법률 문서 내 심볼 간 관계를 추론하기 위해 사용됩니다. 반면 신경망을 활용한 임베딩 기반 모델들은 큰 규모의 데이터로부터 태스크 수행을 위한 잠재 피처를 학습하기 위해 사용됩니다. The differences between
these two methods have caused some problems in
existing works of LegalAI. Interpretable symbolic
models are not effective, and embedding-methods
with better performance usually cannot be interpreted, which may bring ethical issues to the legal
system such as gender bias and racial discrimination. The shortcomings make it difficult to apply
existing methods to real-world legal systems.

We summarize three primary challenges for both
embedding-based and symbol-based methods in
LegalAI: (1) Knowledge Modelling. Legal texts
are well formalized, and there are many domain
knowledge and concepts in LegalAI. How to utilize the legal knowledge is of great significance.
(2) Legal Reasoning. Although most tasks in NLP
require reasoning, the LegalAI tasks are somehow
different, as legal reasoning must strictly follow
the rules well-defined in law. Thus combining predefined rules and AI technology is essential to legal
reasoning. Besides, complex case scenarios and
complex legal provisions may require more sophisticated reasoning for analyzing. (3) Interpretability.
Decisions made in LegalAI usually should be interpretable to be applied to the real legal system.
Otherwise, fairness may risk being compromised.
Interpretability is as important as performance in
LegalAI.

The main contributions of this work are con
cluded as follows: (1) We describe existing works
from the perspectives of both NLP researchers and
legal professionals. Moreover, we illustrate several embedding-based and symbol-based methods
and explore the future direction of LegalAI. (2)
We describe three typical applications, including
judgment prediction, similar case matching, and
legal question answering in detail to emphasize
why these two kinds of methods are essential to
LegalAI. (3) We conduct exhaustive experiments
on multiple datasets to explore how to utilize NLP
technology and legal knowledge to overcome the
challenges in LegalAI. You can find the implementation from github1
. (4) We summarize LegalAI
datasets, which can be regarded as the benchmark
for related tasks. The details of these datasets can
be found from github2 with several legal papers
worth reading.

<br/>

## 2. 임베딩 기반 방법론

First, we describe embedding-based methods in
LegalAI, also named as representation learning.
Embedding-based methods emphasize on representing legal facts and knowledge in embedding
space, and they can utilize deep learning methods
for corresponding tasks.

### 2.1 캐릭터, 단어, 개념 임베딩

Character and word embeddings play a significant
role in NLP, as it can embed the discrete texts into
continuous vector space. Many embedding methods have been proved effective and they are crucial for the
effectiveness of the downstream tasks.

In LegalAI, embedding methods are also essential as they can bridge the gap between texts and
vectors. However, it seems impossible to learn the
meaning of a professional term directly from some
legal factual description. Existing works (Chalkidis
and Kampas, 2019; Nay, 2016) mainly revolve
around applying existing embedding methods like
Word2Vec to legal domain corpora. To overcome
the difficulty of learning professional vocabulary
representations, we can try to capture both grammatical information and legal knowledge in word
embedding for corresponding tasks. Knowledge
modelling is significant to LegalAI, as many results should be decided according to legal rules and
knowledge.

Although knowledge graph methods in the legal domain are promising, there are still two major
challenges before their practical usage. Firstly, the
construction of the knowledge graph in LegalAI
is complicated. In most scenarios, there are no
ready-made legal knowledge graphs available, so
researchers need to build from scratch. In addition, different legal concepts have different representations and meanings under legal systems in
different countries, which also makes it challenging to construct a general legal knowledge graph.
Some researchers tried to embed legal dictionaries (Cvrcek et al. ˇ , 2012), which can be regarded
as an alternative method. Secondly, a generalized
legal knowledge graph is different in the form with
those commonly used in NLP. Existing knowledge
graphs concern the relationship between entities
and concepts, but LegalAI focuses more on the
explanation of legal concepts. These two challenges make knowledge modelling via embedding
in LegalAI non-trivial, and researchers can try to
overcome the challenges in the future.

### 2.2 사전 훈련 언어 모델

Pretrained language models (PLMs) such as
BERT (Devlin et al., 2019) have been the recent
focus in many fields in NLP (Radford et al., 2019;
Yang et al., 2019; Liu et al., 2019a). Given the
success of PLM, using PLM in LegalAI is also a
very reasonable and direct choice. However, there
are differences between the text used by existing
PLMs and legal text, which also lead to unsatisfactory performances when directly applying PLMs
to legal tasks. The differences stem from the terminology and knowledge involved in legal texts. To
address this issue, Zhong et al. (2019b) propose a
language model pretrained on Chinese legal documents, including civil and criminal case documents.
Legal domain-specific PLMs provide a more qualified baseline system for the tasks of LegalAI. We
will show several experiments comparing different
BERT models in LegalAI tasks.

For the future exploration of PLMs in LegalAI,
researchers can aim more at integrating knowledge
into PLMs. Integrating knowledge into pretrained
models can help the reasoning ability between legal concepts. Lots of work has been done on integrating knowledge from the general domain into
models (Zhang et al., 2019; Peters et al., 2019;
Hayashi et al., 2019). Such technology can also be
considered for future application in LegalAI.

<br/>

## 3. 심볼 기반 방법론

In this section, we describe symbol-based methods, also named as structured prediction methods.
Symbol-based methods are involved in utilizing
legal domain symbols and knowledge for the tasks
of LegalAI. The symbolic legal knowledge, such as
events and relationships, can provide interpretability. Deep learning methods can be employed for
symbol-based methods for better performance.

### 3.1 정보 추출(IE)

Information extraction (IE) has been widely studied in NLP. IE emphasizes on extracting valuable
information from texts, and there are many NLP
works which concentrate on IE, including name
entity recognition, relation extraction, and event extraction.

IE in LegalAI has also attracted the interests of
many researchers. To make better use of the particularity of legal texts, researchers try to use ontology
or
global consistency for named
entity recognition in LegalAI. To extract relationship and events from legal documents, re-searchers attempt to apply different NLP technologies, including hand-crafted rules, CRF, joint models like SVM, CNN, GRU, or scale-free identifier network for promising result.
Existing works have made lots of efforts to improve the effect of IE, but we need to pay more
attention to the benefits of the extracted information. The extracted symbols have a legal basis and
can provide interpretability to legal applications,
so we cannot just aim at the performance of methods. Here, we show two examples of utilizing the
extracted symbols for interpretability of LegalAI:

**Relation Extraction and Inheritance Dispute**

Inheritance dispute is a type of cases in Civil Law
that focuses on the distribution of inheritance rights.
Therefore, identifying the relationship between the
parties is vital, as those who have the closest relationship with the deceased can get more assets.
Towards this goal, relation extraction in inheritance
dispute cases can provide the reason for judgment
results and improve performance.

**Event Timeline Extraction and Judgment Prediction of Criminal Case**

In criminal cases,
multiple parties are often involved in group crimes.
To decide who should be primarily responsible for
the crime, we need to determine what everyone has
done throughout the case, and the order of these
events is also essential. For example, in the case of
crowd fighting, the person who fights first should
bear the primary responsibility. As a result, a qualified event timeline extraction model is required for
judgment prediction of criminal cases.

In future research, we need to concern more
about applying extracted information to the tasks
of LegalAI. The utilization of such information
depends on the requirements of specific tasks, and
the information can provide more interpretability.

### 3.2 법률적 요소 추출

In addition to those common symbols in general NLP, LegalAI also has its exclusive symbols,
named legal elements. The extraction of legal elements focuses on extracting crucial elements like
whether someone is killed or something is stolen.
These elements are called constitutive elements of
crime, and we can directly convict offenders based
on the results of these elements. Utilizing these
elements can not only bring intermediate supervision information to the judgment prediction task but also make the prediction results of the model
more interpretable.

Towards a more in-depth analysis of elementbased symbols, Shu et al. (2019) propose a dataset
for extracting elements from three different kinds
of cases, including divorce dispute, labor dispute,
and loan dispute. The dataset requires us to detect
whether the related elements are satisfied or not,
and formalize the task as a multi-label classification
problem. To show the performance of existing
methods on element extraction, we have conducted
experiments on the dataset, and the results can be
found in Table 2.

We have implemented several classical encoding models in NLP for element extraction, including TextCNN (Kim, 2014), DPCNN (Johnson and Zhang, 2017), LSTM (Hochreiter and
Schmidhuber, 1997), BiDAF (Seo et al., 2016),
and BERT (Devlin et al., 2019). We have tried
two different versions of pretrained parameters of
BERT, including the origin parameters (BERT) and
the parameters pretrained on Chinese legal documents (BERT-MS) (Zhong et al., 2019b). From
the results, we can see that the language model
pretrained on the general domain performs worse than domain-specific PLM, which proves the necessity of PLM in LegalAI. For the following parts
of our paper, we will use BERT pretrained on legal
documents for better performance.

From the results of element extraction, we can
find that existing methods can reach a promising
performance on element extraction, but are still not
sufficient for corresponding applications. These elements can be regarded as pre-defined legal knowledge and help with downstream tasks. How to
improve the performance of element extraction is
valuable for further research.

<br/>

## 4. 법률 AI 어플리케이션

In this section, we will describe several typical applications in LegalAI, including Legal Judgment
Prediction, Similar Case Matching and Legal Question Answering. Legal Judgment Prediction and
Similar Case Matching can be regarded as the core
function of judgment in Civil Law and Common
Law system, while Legal Question Answering can
provide consultancy for those who are unfamiliar
with the legal domain. Therefore, exploring these
three tasks can cover most aspects of LegalAI.

### 4.1 법률 판단 예측

Legal Judgment Prediction (LJP) is one of the most
critical tasks in LegalAI, especially in the Civil
Law system. In the Civil Law system, the judgment
results are decided according to the facts and the
statutory articles. One will receive legal sanctions
only after he or she has violated the prohibited acts
prescribed by law. The task LJP mainly concerns
how to predict the judgment results from both the
fact description of a case and the contents of the
statutory articles in the Civil Law system.

As a result, LJP is an essential and representative task in countries with Civil Law system like
France, Germany, Japan, and China. Besides, LJP
has drawn lots of attention from both artificial intelligence researchers and legal professionals. In the
following parts, we describe the research progress
and explore the future direction of LJP.

**관련 연구**

LJP has a long history. Early works revolve around
analyzing existing legal cases in specific circumstances using mathematical or statistical methods (Kort, 1957; Ulmer, 1963; Nagel, 1963; Keown,
1980; Segal, 1984; Lauderdale and Clark, 2012).
The combination of mathematical methods and legal rules makes the predicted results interpretable.

To promote the progress of LJP, Xiao et al.
(2018) have proposed a large-scale Chinese criminal judgment prediction dataset, C-LJP. The dataset
contains over 2.68 million legal documents published by the Chinese government, making C-LJP
a qualified benchmark for LJP. C-LJP contains
three subtasks, including relevant articles, applicable charges, and the term of penalty. The first
two can be formalized as multi-label classification
tasks, while the last one is a regression task. Besides, English LJP datasets also exist (Chalkidis
et al., 2019a), but the size is limited.

With the development of the neural network,
many researchers begin to explore LJP using deep
learning technology (Hu et al., 2018; Wang et al.,
2019; Li et al., 2019b; Liu et al., 2019b; Li et al.,
2019a; Kang et al., 2019). These works can be divided into two primary directions. The first one is
to use more novel models to improve performance.
Chen et al. (2019) use the gating mechanism to
enhance the performance of predicting the term of
penalty. Pan et al. (2019) propose multi-scale attention to handle the cases with multiple defendants.
Besides, other researchers explore how to utilize
legal knowledge or the properties of LJP. Luo et al.
(2017) use the attention mechanism between facts
and law articles to help the prediction of applicable
charges. Zhong et al. (2018) present a topological
graph to utilize the relationship between different
tasks of LJP. Besides, Hu et al. (2018) incorporate
ten discriminative legal attributes to help predict
low-frequency charges.

**실험 및 분석**

To better understand recent advances in LJP, we
have conducted a series of experiments on CLJP. Firstly, we implement several classical text
classification models, including TextCNN (Kim,
2014), DPCNN (Johnson and Zhang, 2017), LSTM (Hochreiter and Schmidhuber, 1997), and
BERT (Devlin et al., 2019). For the parameters of
BERT, we use the pretrained parameters on Chinese
criminal cases (Zhong et al., 2019b). Secondly,
we implement several models which are specially
designed for LJP, including FactLaw (Luo et al.,
2017), TopJudge (Zhong et al., 2018), and Gating
Network (Chen et al., 2019). The results can be
found in Table 4.

From the results, we can learn that most models
can reach a promising performance in predicting
high-frequency charges or articles. However, the
models perform not well on low-frequency labels
as there is a gap between micro-F1 and macro-F1.
Hu et al. (2018) have explored few-shot learning
for LJP. However, their model requires additional
attribute information labelled manually, which is
time-consuming and makes it hard to employ the
model in other datasets. Besides, we can find that
performance of BERT is not satisfactory, as it does
not make much improvement from those models
with fewer parameters. The main reason is that the
length of the legal text is very long, but the maximum length that BERT can handle is 512. According to statistics, the maximum document length is
56, 694, and the length of 15% documents is over
512. Document understanding and reasoning techniques are required for LJP.

Although embedding-based methods can
achieve promising performance, we still need
to consider combining symbol-based with
embedding-based methods in LJP. Take TopJudge
as an example, this model formalizes topological
order between the tasks in LJP (symbol-based
part) and uses TextCNN for encoding the fact
description. By combining symbol-based and
embedding-based methods, TopJudge has achieved
promising results on LJP. Comparing the results between TextCNN and TopJudge, we can find that
just integrating the order of judgments into the
model can lead to improvements, which proves
the necessity of combining embedding-based and
symbol-based methods.

For better LJP performance, some challenges
require the future efforts of researchers: (1) Document understanding and reasoning techniques
are required to obtain global information from extremely long legal texts. (2) Few-shot learning.
Even low-frequency charges should not be ignored
as they are part of legal integrity. Therefore, handling in-frequent labels is essential to LJP. (3) Interpretability. If we want to apply methods to real
legal systems, we must understand how they make
predictions. However, existing embedding-based
methods work as a black box. What factors affected their predictions remain unknown, and this
may introduce unfairness and ethical issues like
gender bias to the legal systems. Introducing legal symbols and knowledge mentioned before will
benefit the interpretability of LJP.

### 4.2 유사 케이스 매칭

In those countries with the Common Law system
like the United States, Canada, and India, judicial
decisions are made according to similar and representative cases in the past. As a result, how to
identify the most similar case is the primary concern in the judgment of the Common Law system.

In order to better predict the judgment results in
the Common Law system, Similar Case Matching
(SCM) has become an essential topic of LegalAI.
SCM concentrate on finding pairs of similar cases,
and the definition of similarity can be various.
SCM requires to model the relationship between
cases from the information of different granularity,
like fact level, event level and element level. In other words, SCM is a particular form of semantic
matching (Xiao et al., 2019), which can benefit the
legal information retrieval.

**관련 연구**

Traditional methods of Information Retrieve (IR)
focus on term-level similarities with statistical models, including TF-IDF (Salton and Buckley, 1988)
and BM25 (Robertson and Walker, 1994), which
are widely applied in current search systems. In
addition to these term matching methods, other researchers try to utilize meta-information (Medin,
2000; Gao et al., 2011; Wu et al., 2013) to capture
semantic similarity. Many machine learning methods have also been applied for IR like SVD (Xu
et al., 2010) or factorization (Rendle, 2010; Kabbur
et al., 2013). With the rapid development of deep
learning technology and NLP, many researchers
apply neural models, including multi-layer perceptron (Huang et al., 2013), CNN (Shen et al.,
2014; Hu et al., 2014; Qiu and Huang, 2015), and
RNN (Palangi et al., 2016) to IR.

There are several LegalIR datasets, including
COLIEE (Kano et al., 2018), CaseLaw (Locke and
Zuccon, 2018), and CM (Xiao et al., 2019). Both
COLIEE and CaseLaw are involved in retrieving
most relevant articles from a large corpus, while
data examples in CM give three legal documents
for calculating similarity. These datasets provide
benchmarks for the studies of LegalIR. Many researchers focus on building an easy-to-use legal
search engine (Barmakian, 2000; Turtle, 1995).
They also explore utilizing more information, including citations (Monroy et al., 2013; Geist, 2009;
Raghav et al., 2016) and legal concepts (Maxwell
and Schafer, 2008; Van Opijnen and Santos, 2017).
Towards the goal of calculating similarity in semantic level, deep learning methods have also been
applied to LegalIR. Tran et al. (2019) propose a
CNN-based model with document and sentence
level pooling which achieves the state-of-the-art
results on COLIEE, while other researchers explore employing better embedding methods for LegalIR (Landthaler et al., 2016; Sugathadasa et al.,
2018).

**실험 및 분석**

To get a better view of the current progress of LegalIR, we select CM (Xiao et al., 2019) for experiments. CM contains 8, 964 triples where each
triple contains three legal documents (A, B, C).
The task designed in CM is to determine whether B or C is more similar to A. We have implemented four different types of baselines: (1) Term
matching methods, TF-IDF (Salton and Buckley,
1988). (2) Siamese Network with two parametershared encoders, including TextCNN (Kim, 2014),
BiDAF (Seo et al., 2016) and BERT (Devlin et al.,
2019), and a distance function. (3) Semantic matching models in sentence level, ABCNN (Yin et al.,
2016), and document level, SMASH-RNN (Jiang
et al., 2019). The results can be found in Table 5.

From the results, we observe that existing neural models which are capable of capturing semantic information outperform TF-IDF, but the performance is still not enough for SCM. As Xiao
et al. (2019) state, the main reason is that legal
professionals think that elements in this dataset
define the similarity of legal cases. Legal professionals will emphasize on whether two cases have
similar elements. Only considering term-level and
semantic-level similarity is insufficient for the task.

For the further study of SCM, there are two directions which need future effort: (1) Elementalbased representation. Researchers can focus
more on symbols of legal documents, as the similarity of legal cases is related to these symbols
like elements. (2) Knowledge incorporation. As
semantic-level matching is insufficient for SCM,
we need to consider about incorporating legal
knowledge into models to improve the performance
and provide interpretability.

### 4.3 법률 Q&A

Another typical application of LegalAI is Legal
Question Answering (LQA) which aims at answering questions in the legal domain. One of the most
important parts of legal professionals’ work is to
provide reliable and high-quality legal consulting
services for non-professionals. However, due to
the insufficient number of legal professionals, it is
often challenging to ensure that non-professionals can get enough and high-quality consulting services, and LQA is expected to address this issue.

In LQA, the form of questions varies as some
questions will emphasize on the explanation of
some legal concepts, while others may concern
the analysis of specific cases. Besides, questions
can also be expressed very differently between professionals and non-professionals, especially when
describing domain-specific terms. These problems
bring considerable challenges to LQA, and we conduct experiments to demonstrate the difficulties of
LQA better in the following parts.

**관련 연구**

In LegalAI, there are many datasets of question answering. Duan et al. (2019) propose CJRC, a legal
reading comprehension dataset with the same format as SQUAD 2.0 (Rajpurkar et al., 2018), which
includes span extraction, yes/no questions, and
unanswerable questions. Besides, COLIEE (Kano et al., 2018) contains about 500 yes/no questions.
Moreover, the bar exam is a professional qualification examination for lawyers, so bar exam
datasets (Fawei et al., 2016; Zhong et al., 2019a)
may be quite hard as they require professional legal
knowledge and skills.

In addition to these datasets, researchers have
also worked on lots of methods on LQA. The rulebased systems (Buscaldi et al., 2010; Kim et al.,
2013; Kim and Goebel, 2017) are prevalent in early
research. In order to reach better performance,
researchers utilize more information like the explanation of concepts (Taniguchi and Kano, 2016;
Fawei et al., 2015) or formalize relevant documents
as graphs to help reasoning (Monroy et al., 2009,
2008; Tran et al., 2013). Machine learning and
deep learning methods like CRF (Bach et al., 2017),
SVM (Do et al., 2017), and CNN (Kim et al., 2015)
have also been applied to LQA. However, most
existing methods conduct experiments on small
datasets, which makes them not necessarily applicable to massive datasets and real scenarios.

**실험 및 분석**

We select JEC-QA (Zhong et al., 2019a) as the
dataset of the experiments, as it is the largest
dataset collected from the bar exam, which guarantees its difficulty. JEC-QA contains 28, 641
multiple-choice and multiple-answer questions, together with 79, 433 relevant articles to help to answer the questions. JEC-QA classifies questions
into knowledge-driven questions (KD-Questions)
and case-analysis questions (CA-Questions) and
reports the performances of humans. We implemented several representative question answering models, including BiDAF (Seo et al., 2016),
BERT (Devlin et al., 2019), Co-matching (Wang
et al., 2018), and HAF (Zhu et al., 2018). The
experimental results can be found in Table 6.

From the experimental results, we can learn the models cannot answer the legal questions well compared with their promising results in open-domain
question answering and there is still a huge gap
between existing models and humans in LQA.

For more qualified LQA methods, there are several significant difficulties to overcome: (1) Legal multi-hop reasoning. As Zhong et al. (2019a)
state, existing models can perform inference but not
multi-hop reasoning. However, legal cases are very
complicated, which cannot be handled by singlestep reasoning. (2) Legal concepts understanding. We can find that almost all models are better
at case analyzing than knowledge understanding,
which proves that knowledge modelling is still challenging for existing methods. How to model legal
knowledge to LQA is essential as legal knowledge
is the foundation of LQA.

<br/>

## 5. 결론

In this paper, we describe the development status
of various LegalAI tasks and discuss what we can
do in the future. In addition to these applications
and tasks we have mentioned, there are many other
tasks in LegalAI like legal text summarization and
information extraction from legal contracts. Nevertheless, no matter what kind application is, we
can apply embedding-based methods for better performance, together with symbol-based methods for
more interpretability.

Besides, the three main challenges of legal tasks
remain to be solved. Knowledge modelling, legal
reasoning, and interpretability are the foundations
on which LegalAI can reliably serve the legal domain. Some existing methods are trying to solve
these problems, but there is still a long way for
researchers to go.

In the future, for these existing tasks, researchers
can focus on solving the three most pressing challenges of LegalAI combining embedding-based
and symbol-based methods. For tasks that do not
yet have a dataset or the datasets are not large
enough, we can try to build a large-scale and highquality dataset or use few-shot or zero-shot methods to solve these problems.

Furthermore, we need to take the ethical issues
of LegalAI seriously. Applying the technology
of LegalAI directly to the legal system will bring
ethical issues like gender bias and racial discrimination. The results given by these methods cannot
convince people. To address this issue, we must
note that the goal of LegalAI is not replacing the legal professionals but helping their work. As a
result, we should regard the results of the models
only as a reference. Otherwise, the legal system
will no longer be reliable. For example, professionals can spend more time on complex cases and
leave the simple cases for the model. However, for
safety, these simple cases must still be reviewed. In
general, LegalAI should play as a supporting role
to help the legal system.
